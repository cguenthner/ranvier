(ns ranvier.core
  (:require [clojure.core.async :as async]
            [clojure.java.io :as io]
            [clojure.pprint :refer [pprint]]
            [clojure.set :as set]
            [clojure.string :as string]
            [ranvier.utils :as u :refer [def- -?> -?>>]]
            [tensure.core :as m])
  (:refer-clojure :exclude [min max]))

(defn set-rng-seed!
  "Sets the seed of the random number generator (RNG) used for random parameter initialization."
  [seed-value]
  (m/set-rng-seed! seed-value))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Computation graph node definition
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; Node definitions
(def node-types #{:op :const :input})

(declare const input)

; TODO: Use metadata to define type as a node.
(defn node?
  "Returns `true` iff `x` is a computation graph node."
  [x]
  (boolean (and (map? x) (node-types (:type x)))))

(declare is-input? update-node get-node-data)
(defn construct-operand
  ([operand]
   (construct-operand operand true))
  ([operand nd?]
   (cond (node? operand) (if (and (is-input? operand)
                                  ; TODO: Clean this up.
                                  (not (contains? (get-node-data operand) :is-nd?)))
                           (update-node operand :data (assoc (get-node-data operand)
                                                        :is-nd? nd?))
                           operand)
         (keyword? operand) (input operand nd?)
         nd? (cond (m/array? operand) (const operand)
                   (or (number? operand)
                       (sequential? operand)) (const (m/array operand))
                   :else (u/throw-str "Invalid operand: " operand))
         :else (const operand))))

(declare get-upstream-nodes get-node-name)
(defn make-node
  "Returns a new computation graph node. `opts` must contain:

    - `:type` - the type of node (`:op`, `:const`, or `:input`)

  It may additionally contain:

    - `:value` - the value of a `:const` node
    - `:name` - the keyword name of the node (autogenerated if not provided)
    - `:op` - keyword indicating the operation (for `:op` nodes)
    - `:operands` - vector of operand nodes
    - `forward-fn` - for `:op` nodes, a function `f` for which `(apply f operands)` returns the value of
      the node:
    - `backward-fns` - for `:op` nodes, a seq of functions `f1`, `f2`, `f3`, ..., `fi` where
      `(apply fi dy v1 v2 v3 ...)` returns the gradient `dv1/d-out`, where `d-out` is the final output of the
      graph, `y` is the value of this node, `dy` is the gradient of this node (`dy/d-out`), and
      `v1`, `v2`, `v3`, ..., `vi` are the values of the operands
    - `initializer` -  for `:input` nodes, a function `f` for which `(f input-map)` returns the initial value
      of the node (on the first run of the graph if its value is not provided); `input-map` is a map of node
      names to values for nodes that have already been evaluated.
    - `:private-nodes` - a set of names of nodes that are private to a node's subtree (i.e. that cannot
      appear in the graph outside the nodes opearnds)
    - `data` - an arbitrary map of additional data"
  [opts]
  (u/check-key-validity opts [:type] [:value :name :op :operands :forward-fn
                                      :backward-fns :initializer :private-nodes :data])
  (let [{:keys [type value name op operands forward-fn
                backward-fns initializer private-nodes data]} opts]
    (when (not (node-types type))
      (u/throw-str "Attempted to construct a node with invalid type '" type "'. Allowed types are "
                   (vec node-types) "."))
    (when-not (or (nil? name)
                  (keyword? name))
      (u/throw-str "Invalid node name: '" name "'."))
    {:type type
     :name (keyword (or name
                        (gensym (or op "const"))))
     :value value
     :op op
     ; TODO: Validate that `operands` are all nodes.
     :operands operands
     :forward-fn forward-fn
     :backward-fns backward-fns
     :initializer initializer
     :upstream-nodes (->> (mapv get-upstream-nodes operands)
                          (apply set/union (set (mapv get-node-name operands))))
     :private-nodes private-nodes
     :data data}))

(defn update-node
  "Given a computation graph node and key/value pairs where the keys are node properties (keys accepted by
  the `opts` argument to `make-node`), returns a computation graph node with those properties updated to the
  provided values."
  [node & updates]
  (with-meta (make-node (-> (apply assoc node updates)
                            (dissoc :upstream-nodes)))
    (meta node)))

(defn is-const?
  "Returns true iff `node` is a constant node."
  [node]
  (= (:type node) :const))

(defn is-input?
  "Returns true iff `node` is an input node."
  [node]
  (= (:type node) :input))

(defn is-op?
  "Returns true iff `node` is an operation node."
  [node]
  (= (:type node) :op))

(defn get-node-type
  "Returns the type of a `node` (`:const`, `:input`, or `:op`)."
  [node]
  (:type node))

(defn get-node-name
  "Returns the name of a `node` (a keyword)."
  [node]
  (:name node))

(defn get-node-value
  "Returns the value of a constant `node`, or else `nil`."
  [node]
  (:value node))

(defn get-node-op
  "Returns the name of the operation for an op `node`, or else `nil`."
  [node]
  (:op node))

(defn get-node-operands
  "Returns a seq of operand nodes for an op `node`, or else `nil`."
  [node]
  (:operands node))

(defn get-node-forward-fn
  "Returns the forward propagation function of an op `node`, or else `nil`."
  [node]
  (:forward-fn node))

(defn get-node-backward-fns
  "Returns the backpropagation function of an op `node`, or else `nil`."
  [node]
  (:backward-fns node))

(defn get-node-initializer
  "Returns the initializer function of a `node`, or else `nil`."
  [node]
  (:initializer node))

(defn get-upstream-nodes
  "Returns a seq all 'upstream' nodes of the provided `node`, i.e. all those nodes that are operands,
  operands of operands, etc. The term 'upstream' is from the perpsective of forward propagation."
  [node]
  (:upstream-nodes node))

(defn get-private-nodes
  "Returns a seq of nodes that are 1) upstream of `node` and 2) 'private' (i.e. that cannot be used in the
  graph outside of the subranch upstream of `node`."
  [node]
  (:private-nodes node))

; TODO: Consider keeping track of old name and checking in `validate-graph` that no nodes are using the old
; name.
(defn named
  "Creates a new node that's identical to `node` but named `new-name`."
  [new-name node]
  (update-node (construct-operand node) :name new-name))

(defn get-node-data
  "Returns a map of `data` associated with `node`."
  [node]
  (:data node))

(defn assoc-node-data
  "Assoc's key/value pair `k`/`v` into `node`'s data map."
  [node k v]
  (update-node node
               :data (assoc (get-node-data node) k v)))

(defn get-all-graph-nodes
  "Returns a seq of all nodes in the provided computation graph."
  [graph]
  (let [traverse (fn traverse
                   [node seen-nodes]
                   (reduce (fn [now-seen operand]
                             (if (now-seen operand)
                               now-seen
                               (set/union now-seen #{operand} (traverse operand now-seen))))
                           seen-nodes
                           (get-node-operands node)))]
    (seq (traverse graph #{graph}))))

(defn filter-graph-nodes
  "Given a root `node` of a computation graph, returns a seq of nodes in that computation graph for which
  `(f node)` is truthy."
  [f node]
  (when (f node)
    (->> (get-node-operands node)
         (mapcat (partial filter-graph-nodes f))
         (cons node))
    #_[node]))

(defn get-all-graph-inputs
  "Given a computation `graph` (the root node of the graph), returns a seq of all the input nodes."
  [graph]
  (filter is-input? (get-all-graph-nodes graph)))

(defn get-all-graph-input-names
  "Returns a seq of (keyword) names for all of `graph`s input nodes, given the root node of the graph."
  [graph]
  (->> (get-all-graph-inputs graph)
       (map get-node-name)))

(def- get-graph-input-types
  "For graph `g`, returns a map of input name to a boolean that's true if that input is used in at least
  one location in the graph where a tensor is expected."
  (memoize
    (fn
      [g]
      (reduce (fn [m input-node]
                (update m (get-node-name input-node) #(or % (:is-nd? (get-node-data input-node)))))
              {}
              (get-all-graph-inputs g)))))

(defn- prepare-input-map
  "Prepares input map `m` for graph `g` by identifying the expected value type (tensor vs. not) for each
  input, and constructing tensors where necessary. This enables the convenience of feeding tensor input
  values like [[1 2 3] [4 5 6]] without explicitly constructing an array from it."
  [g m]
  (let [input-name->nd? (get-graph-input-types g)]
    #_(println input-name->nd?)
    (reduce (fn [m input-name]
              (let [input-val (get m input-name)]
                (if (and (get input-name->nd? input-name)
                         ; We allow nil values for tensor inputs, because they may be initialized at run-time.
                         (not (or (m/array? input-val) (nil? input-val))))
                  (update m input-name m/array)
                  m)))
            m
            (keys input-name->nd?))))

(defn const
  "Returns a new constant node with value `val` and (optionally) name `name` (autogenerated if not provided)."
  ([val]
   (const val nil))
  ([val name]
   (make-node {:type :const
               :name name
               :value val})))

(defn input
  "Cosntructs an input node from:

    - `name` (required) - keyword name for this node
    - `is-nd?` (optional, default `nil`) - boolean indicating if this input is a tensor. If `true`, values
      provided for this input (including numbers and vectors of vectors/numbers) will automatically be
      converted into tensors. If `false`, the input values will be unaltered. If `nil`, they will be converted
      into a tensor automatically if they are used in an op in the graph that expects a tensor; otherwise
      they'll remain unaltered.
    - `initializer` (optional) - function that must receive a map of node names to values for nodes that have
      been evaluated thus far and that must return the initialized value for this node.
    - `data` (optional) - map of arbitrary key-value pairs to associate with this node.

  The arguments can be provided in any of the following combinations:
    (input name)
    (input name initializer)
    (input name is-nd?)
    (input name data)
    (input name initializer is-nd?)
    (input name initializer data)
    (input name is-nd data)
    (input name initiailizer is-nd? data)"
  ([name]
   (input name nil))
  ([name initializer-is-nd-or-data]
   (let [[initializer-or-is-nd data] (if (map? initializer-is-nd-or-data)
                                       [nil initializer-is-nd-or-data]
                                       [initializer-is-nd-or-data {}])]
     (input name initializer-or-is-nd data)))
  ([name initializer-or-is-nd is-nd-or-data]
   (let [[initializer is-nd?] (cond (fn? initializer-or-is-nd) [initializer-or-is-nd nil]
                                    (or (boolean? initializer-or-is-nd)
                                        (nil? initializer-or-is-nd)) [nil initializer-or-is-nd]
                                    :else (u/throw-str "Unexpected argument type: '" (type initializer-or-is-nd)
                                                       "'. Expected an initializer function or a boolean "
                                                       "indicating if input is a tensor."))
         [is-nd? data] (cond (and (nil? is-nd?) (boolean? is-nd-or-data)) [is-nd-or-data {}]
                             (map? is-nd-or-data) [is-nd? is-nd-or-data]
                             :else (u/throw-str "Unexpected argument type: '" (type is-nd-or-data)
                                                "'. Expected a boolean indicating if input is a tensor "
                                                "or a map of data."))]
     (input name initializer is-nd? data)))
  ([name initializer is-nd? data]
   (make-node {:type :input
               :name name
               :initializer initializer
               :data (if (nil? is-nd?)
                       data
                       (assoc data :is-nd? is-nd?))})))

(defn- input-used-out-of-scope?
  "Given a map of node names to seqs of nodes private to the subbranch for which the named node is root,
  returns the name of the first node that's used out of scope, or else `nil`."
  [g parent->private-nodes]
  (if (is-input? g)
    (reduce (fn [out-of-scope-node-name private-nodes]
              (when (private-nodes (get-node-name g))
                (reduced (get-node-name g))))
            nil
            (vals parent->private-nodes))
    (let [new-parent->private-nodes (dissoc parent->private-nodes (get-node-name g))]
      (->> (get-node-operands g)
           (some #(input-used-out-of-scope? % new-parent->private-nodes))))))

(def- validate-graph
  "A function that takes one argument, the root node of a computation graph, performs a series fo checks of
   the validity of the graph, and throws an `Exception` if the graph is invalid in some way. Returns `nil`."
  (memoize
    (fn [g]
      (let [all-nodes (get-all-graph-nodes g)
            parent->private-nodes (->> all-nodes
                                       (keep (fn [n]
                                               (when-let [private-nodes (get-private-nodes n)]
                                                 [(get-node-name n) private-nodes])))
                                       (into {}))]
        (doseq [[name nodes] (->> all-nodes
                                  ; We allow nodes to have different values for `is-nd?`, because some 'copies'
                                  ; of a node can get this tag and others not depending on where they are used.
                                  (mapv #(update % :data dissoc :is-nd?))
                                  distinct
                                  (group-by get-node-name))]
          (when (> (count nodes) 1)
            (u/throw-str "Invalid graph. Node name '" name "' refers to multiple distinct nodes: '" nodes "'.")))
        (when-let [out-of-scope-node-name (input-used-out-of-scope? g parent->private-nodes)]
          (u/throw-str "Invalid graph. Node '" out-of-scope-node-name "' is used out of scope."))))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Operations
;;
;; Operations (ops) are the ranvier primitive. Each op requires a definition of 1) its value calculation,
;; and 2) the calculations of the gradients of its operands with respect to the graph output. However, some
;; ops (e.g. the comparison and logical ops) are "non-backpropagating" (i.e. they don't contribute to
;; gradient calculations of any upstream variables). Internally, ops are just functions that take a node and
;; return a node. Some ops defined here are just functions that express the op in terms of a more primitive
;; (usually private) op. For instance, `add` is a public function that handles any numbers of operands via
;; dependence on `add-2`, which is private and handles only two operands. `add` is considered an op only
;; because the addition operation itself is considered primitive. Ops cannot do anything with a node
;; that would invalidate the gradient calculation via backpropagation (e.g. they cannot update a node value
;; in a way that would change the gradient without also updating the gradient functions).
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(defmacro defop
  "Define an operation. Example usage:

  ```
    (defop increment  ; Op name
      \"Given scalar `n`, returns `n + 1`.\"  ; Optional docstring
      [n]  ; Operand list
      :v (+ n 1)  ; Name and expression for the operation result (v = n + 1)
      :dn dv  ; Name and expression for the gradient of the operand; dv is the gradient of v
     )
  ```

  The first element of the body (e.g. `:v`) is a keyword name for the value returned by the following
  expression (e.g. `(+ n 1)`). This is followed by, alternately, the keyword names (e.g. `:dn`) and
  expressions (e.g. `dv`) for the gradients of each operand. The derivative name must match the name of the
  argument in the argument list, prefixed by a `d`. A symbol for the backpropagated result gradient (the name
  of the result prefixed with `d`--e.g. `dv`) is available in the context of the derivative
  expression."
  [name & args]
  (when-not (symbol? name)
    (u/throw-str "Invalid definition provided to `defop`. First argument must be a symbol, not '" name "'."))
  (let [[docstring args] (if (string? (first args))
                           [(first args) (rest args)]
                           ["" args])
        [operands fn-defs] [(first args) (rest args)]
        _ (when-not (and (vector? operands)
                         (every? #(symbol? %) operands))
            (u/throw-str "Invalid operand list provied to `defop`: '" operands "'."))
        throw-invalid-fn-defs (fn [] (u/throw-str "Invalid function definitions provided "
                                                  "to `defop`: '" fn-defs "'."))
        _ (when-not (even? (count fn-defs))
            (throw-invalid-fn-defs))
        partitioned-fn-defs (map vec (partition 2 fn-defs))
        _ (when-not (every? #(-> % first keyword?) partitioned-fn-defs)
            (throw-invalid-fn-defs))
        y->fn (into {} partitioned-fn-defs)
        operand-grad-keys (map #(keyword (str "d" %)) operands)
        [y-key forward-fn-body] (first y->fn)
        dy-sym (symbol (str "d" (clojure.core/name y-key)))
        back-fn-arglist (vec (concat [dy-sym] operands))
        back-fns (vec (map (fn [operand-grad-key]
                             (when-let [back-fn-body (get y->fn operand-grad-key)]
                               `(fn ~back-fn-arglist ~back-fn-body)))
                           operand-grad-keys))]
    `(defn ~name
       ~docstring
       ~operands
       (make-node
         {:type :op
          :op ~(str name)
          :operands ~(mapv #(list 'ranvier.core/construct-operand % (-> % meta :nd)) operands)
          :forward-fn (fn ~operands ~forward-fn-body)
          :backward-fns ~back-fns}))))

(defmacro defop-
  "Like `defop` but defines a private operation (available only in the namespace where it's defined)."
  [name & args]
  (list* `defop (with-meta name (assoc (meta name) :private true)) args))

;;
;; Helper functions used in op definitions
;;

(defn- reverse-broadcast
  "Given a target matrix shape and a `val` array of the same or higher dimension, this returns a new array
  representing `val` 'reverse broadcast' to the target shape. If `val` is a gradient matrix of an operation
  that included broadcasting, then this can be used during backpropagation to determine the gradient for the
  lower dimensional argument given the gradient of the higher-dimensional result. For instance, due to
  broadcasting, `(m/add [[1 2] [3 4]] c)` will return x = [[6 9] [8 9]] if c = 5. If dJ/dx = [[10 20] [30 40]],
  then `(reverse-broadcast nil [[10 20] [30 40]])` will give dJ/dc = 100."
  [val target-shape]
  (cond (= (seq (m/shape val)) (seq target-shape)) val
        (and (= (apply * target-shape) 1)
             (not (m/scalar? val))) (m/reshape (m/esum val) target-shape)
        :else (let [src-rank (m/rank val)
                    target-rank (count target-shape)
                    src-shape (m/shape val)
                    broadcasted-dims (->> (partition target-rank 1 src-shape)
                                          reverse
                                          (keep-indexed (fn [i src-subshape]
                                                          (when (every? (fn [[s t]] (or (= t s) (= t 1)))
                                                                        (u/zip src-subshape target-shape))
                                                            (let [leading-dims (range (- src-rank target-rank i))
                                                                  leading-dim-count (count leading-dims)]
                                                              (concat leading-dims
                                                                      (keep-indexed #(when (= 1 %2)
                                                                                       (+ %1 leading-dim-count))
                                                                                    target-shape)
                                                                      (range (- src-rank i) src-rank))))))
                                          first)]
                (when-not broadcasted-dims
                  (u/throw-str "Cannot reverse broadcast tensor of shape '" src-shape "' to shape '"
                               target-shape "'."))
                (-> (m/sum-along val broadcasted-dims true)
                    (m/reshape target-shape)))))

(defn- valid-dim?
  "Returns true `iff` `dim` is a valid dimension index."
  [dim]
  (nat-int? dim))

;;
;; Special ops
;;

(defop- tensor-if
  "Manages control flow. Handling of this op is built-in to `forward` and `backward`, so the op itself
  doesn't do anything."
  [^:nd flag ^:nd if-true ^:nd if-false]
  :v nil)

;;
;; Arithmetic operators
;;

(defop- add-2
  "a + b"
  [^:nd a ^:nd b]
  :v (m/add a b)
  :da (reverse-broadcast dv (m/shape a))
  :db (reverse-broadcast dv (m/shape b)))

(defn add
  "Addition of arbitrary number of operands"
  [& operands]
  (case (count operands)
    0 (const 0)
    1 (first operands)
    (reduce #(add-2 %1 %2) operands)))

(defop negate
  "-a"
  [^:nd a]
  :v (m/negate a)
  :da (m/negate dv))

(defop- sub-2
  "a - b"
  [^:nd a ^:nd b]
  :v (m/sub a b)
  :da (reverse-broadcast dv (m/shape a))
  :db (reverse-broadcast (m/negate dv) (m/shape b)))

(defn sub
  "Subtraction of arbitrary number of operands"
  [& operands]
  (case (count operands)
    0 (const 0)
    1 (negate (first operands))
    (reduce #(sub-2 %1 %2) operands)))

(defop- mul-2
  "a * b"
  [^:nd a ^:nd b]
  :v (m/mul a b)
  :da (reverse-broadcast (m/mul dv b) (m/shape a))
  :db (reverse-broadcast (m/mul dv a) (m/shape b)))

(defn mul
  "Multiplication of arbitrary number of operands"
  [& operands]
  (case (count operands)
    0 (const 1)
    1 (first operands)
    (reduce #(mul-2 %1 %2) operands)))

(defop- div-2
  "a / b"
  [^:nd a ^:nd b]
  :v (m/div a b)
  :da (reverse-broadcast (m/div dv b) (m/shape a))
  ; TODO: Move the (m/array 2) out to avoid reconstructing it everytime?
  :db (reverse-broadcast (m/div (m/mul a (m/negate dv)) (m/pow b (m/array 2))) (m/shape b)))

(defn div
  "Division of arbitrary number of operands"
  [first-operand & rest-operands]
  (if (seq rest-operands)
    (reduce #(div-2 %1 %2) first-operand rest-operands)
    (div-2 1 first-operand)))

(defop pow
  "a ^ b"
  [^:nd a ^:nd b]
  :v (m/pow a b)
  :da (reverse-broadcast (m/mul dv b (m/pow a (m/sub b (m/array 1)))) (m/shape a))
  :db (reverse-broadcast (m/mul dv (m/pow a b) (m/log a)) (m/shape b)))

(defn- mmul-db
  "For v = a . b, backpropagates the gradient of some output with respect to v (`dv`) and returns that output
  with respect to b (`db`)."
  [dv a b]
  (cond (or (m/scalar? a)
            (m/scalar? b)) (reverse-broadcast (m/mul a dv) (m/shape b))
        (m/vec? a) (m/outer-product a dv)
        (m/matrix? a) (m/mmul (m/transpose a) dv)
        :else (let [result (m/zeros (m/shape b))]
                (mapv #(m/add! result (mmul-db %1 %2 b)) (m/slices dv) (m/slices a))
                result)))

(defn- mmul-da
  "For v = a . b, backpropagates the gradient of some output with respect to v (`dv`) and returns that output
  with respect to b (`da`)."
  [dv a b]
  (cond (or (m/scalar? a)
            (m/scalar? b)) (reverse-broadcast (m/mul b dv) (m/shape a))
        (m/vec? b) (m/outer-product dv b)
        (m/matrix? b) (m/mmul dv (m/transpose b))
        :else (m/transpose (mmul-db (m/transpose dv) (m/transpose b) (m/transpose a)))))

; TODO: Make mmul that can handle more than two arguments.
(defop mmul
  "Computes:

    - the inner product of `a` and `b` when `a` and `b` are matrices or vectors
    - the inner product of the highest dimension of the tensor with the vector when one argument is a tensor
      and the other is a vector
    - the tensor dot product of `a` and `b` when either `a` or `b` is a tensor and the other is a tensor
      or matrix
    - element-wise product when `a` or `b` is a scalar

  When a and b are tensors of shape [a b c d e] and [f g h i j], then the product will be a tensor of shape
  [a b c d g e h i j] - i.e. the result is produced by multiplying every d x e matrix from `a` by every
  f x g matrix from `b` to get a * b * c * h * i * j d x g products."
  [^:nd a ^:nd b]
  :v (m/mmul a b)
  :da (mmul-da dv a b)
  :db (mmul-db dv a b))

(defop abs
  "|a|"
  [^:nd a]
  :v (m/abs a)
  :da (m/mul dv (m/sub (m/gt a (m/array 0)) (m/lt a (m/array 0)))))

(defop exp
  "e ^ a"
  [^:nd a]
  :v (m/pow (m/array Math/E) a)
  :da (m/mul dv (m/pow (m/array Math/E) a)))

(defop log
  "ln a (natural--base e--logarithm of a)"
  [^:nd a]
  :v (m/log a)
  :da (m/mul dv (m/div (m/array 1) a)))

;;
;; Min/max
;;

(defop- max-2
  "Elementwise maximum of a and b"
  [^:nd a ^:nd b]
  :v (m/max a b)
  :da (reverse-broadcast (m/mul dv (m/ge a b)) (m/shape a))
  :db (reverse-broadcast (m/mul dv (m/ge b a)) (m/shape b)))

(defn max
  "Elementwise maximum of arbitrary number of operands"
  [first-operand & rest-operands]
  (if (seq rest-operands)
    (reduce #(max-2 %1 %2) first-operand rest-operands)
    first-operand))

(defop- min-2
  "Elementwise minimum of a and b"
  [^:nd a ^:nd b]
  :v (m/min a b)
  :da (reverse-broadcast (m/mul dv (m/le a b)) (m/shape a))
  :db (reverse-broadcast (m/mul dv (m/le b a)) (m/shape b)))

(defn min
  "Elementwise minimum of arbitrary number of operands"
  [first-operand & rest-operands]
  (if (seq rest-operands)
    (reduce #(min-2 %1 %2) first-operand rest-operands)
    first-operand))

;;
;; Shape-changing / aggregation operations
;;

; TODO: Make these more efficient. The min-along and max-along operators compute the min-along and max-along
; results during both forward and backward propagation. If these could be cached, this recomputation could
; be avoided.
(defop esum
  "Returns the scalar sum of all the elements of `a`."
  [^:nd a]
  :v (m/esum a)
  :da (m/filled (m/shape a) dv))

(defop emax
  "Returns the maximum element in `a`."
  [^:nd a]
  :v (m/emax a)
  :da (m/mul dv (m/eq a (m/emax a))))

(defop emin
  "Returns the minimum element in `a`."
  [^:nd a]
  :v (m/emin a)
  :da (m/mul dv (m/eq a (m/emin a))))

(defn- normalize-axes
  "Given tensor `nd` and `axes` (`nil`, a vector of axis indices, or a single axis index), returns a
  normalized seq of axis indices."
  [nd axes]
  (cond (number? axes) [axes]
        (nil? axes) (range (m/rank nd))
        :else axes))

(defn- unreduce-axes
  "Given a `dy` that is the same shape as the result of reducing `nd` across `axes`, returns a tensor in which
  `dy` has been tiled to the shape of `nd`. Each value at (i, j, ...) in the result is the value in `dy` to
  which (i, j, ...) in `nd` contributed."
  [dy nd axes]
  (let [axes-set (->> (normalize-axes nd axes)
                      (into #{}))
        target-dy-shape (map-indexed (fn [i nd-size]
                                       (if (axes-set i)
                                         1
                                         nd-size))
                                     (m/shape nd))]
    (-> (m/reshape dy target-dy-shape)
        (m/broadcast-like nd))))

; Sum along axes
(defop- sum-along-op
  [^:nd nd axes collapse]
  :v (m/sum-along nd (or axes (range (m/rank nd))) collapse)
  :dnd (unreduce-axes dv nd axes))

(defn sum-along
  "Returns the sum across some dimensions of the input tensor. `options` can include:

    - `:axis` - an integer or a vector of integers of the axes to sum across
    - `:collapse` (optional, default true) - a boolean that if `true` indicates the summed dimension should
      be removed, and, if false, indicates it should be kept with a size of 1."
  ([nd]
   (sum-along nd {}))
  ([nd {:keys [axis collapse] :as options}]
   (u/check-key-validity options [] [:axis :collapse])
   (sum-along-op nd (:axis options) (get options :collapse true))))

; TODO: Move this to tensure?
(defn tensure-max-mask
  "Returns a tensor of the same shape of `nd` where all elements that are the max along `axes` (a single
  axis or vector of axes) are 1 and the remaining elements are 0."
  [nd axes]
  (let [axes-vec (normalize-axes nd axes)]
    (-> (m/max-along nd axes-vec false)
        (unreduce-axes nd axes-vec)
        (m/eq nd))))

(defop max-mask
  "Returns a tensor of the same shape of `nd` where all elements that are the max along `axes` (a single
  axis or vector of axes) are 1 and the remaining elements are 0. Non-backpropagating."
  [^:nd nd axes]
  :v (tensure-max-mask nd axes))

(defop- max-along-op
  [^:nd nd axes collapse]
  :v (m/max-along nd (or axes (range (m/rank nd))) collapse)
  :dnd (m/mul (tensure-max-mask nd axes)
              (unreduce-axes dv nd axes)))

(defn max-along
  "Returns the maximum across some dimensions of the input tensor. `options` can include:

    - `:axis` - an integer or a vector of integers of the axes to sum across
    - `:collapse` (optional, default true) - a boolean that if `true` indicates the summed dimension should
      be removed, and, if false, indicates it should be kept with a size of 1."
  ([nd]
   (max-along nd {}))
  ([nd {:keys [axis collapse] :as options}]
   (u/check-key-validity options [] [:axis :collapse])
   (max-along-op nd (:axis options) (get options :collapse true))))

; Min along axes
(defn tensure-min-mask
  "Returns a tensor of the same shape of `nd` where all elements that are the min along `axes` (a single
  axis or vector of axes) are 1 and the remaining elements are 0."
  [nd axes]
  (let [axes-vec (normalize-axes nd axes)]
    (-> (m/min-along nd axes-vec false)
        (unreduce-axes nd axes-vec)
        (m/eq nd))))

(defop min-mask
  "Returns a tensor of the same shape of `nd` where all elements that are the min along `axes` (a single
  axis or vector of axes) are 1 and the remaining elements are 0. Non-backpropagating."
  [^:nd nd axes]
  :v (tensure-min-mask nd axes))

(defop- min-along-op
  [^:nd nd axes collapse]
  :v (m/min-along nd (or axes (range (m/rank nd))) collapse)
  :dnd (m/mul (tensure-min-mask nd axes)
              (unreduce-axes dv nd axes)))

(defn min-along
  "Returns the minimum across some dimensions of the input tensor. `options` can include:

    - `:axis` - an integer or a vector of integers of the axes to sum across
    - `:collapse` (optional, default true) - a boolean that if `true` indicates the summed dimension should
      be removed, and, if false, indicates it should be kept with a size of 1."
  ([nd]
   (min-along nd {}))
  ([nd {:keys [axis collapse] :as options}]
   (u/check-key-validity options [] [:axis :collapse])
   (min-along-op nd (:axis options) (get options :collapse true))))

(defop transpose
  "Returns the transpose of `a`"
  [^:nd a]
  :v (m/transpose a)
  :da (m/transpose dv))

(defop element-count
  "Returns the number of scalar elements in `nd` as a Tensure object."
  [^:nd nd]
  :v (m/array (m/ecount nd)))

(defn- size-along-axes
  "Returns the size of dimension(s) of tensor `nd`, given a single axis index or vector of axis indices. If
  multiple axes are specified, the returned value is the product of their sizes."
  [nd axes]
  (when-not (and (sequential? axes)
                 (every? valid-dim? axes))
    (u/throw-str "Invalid axes: '" axes "'."))
  (if (and (m/scalar? nd) (= [0] axes))
    1
    (let [shape (m/shape nd)
          dim-count (m/dimensionality nd)]
      (reduce (fn [size axis]
                (if (>= axis dim-count)
                  (u/throw-str "Cannot get size of axis '" axis "' for a tensor of shape '" shape "'.")
                  (* size (m/dimension-count nd axis))))
              1
              axes))))

(defop- size-axes
  "Returns the product of the sizes of all axes in `axes`, a vector of axis indices or else a single axis"
  [^:nd nd axes]
  :v (m/array (size-along-axes nd axes)))

(defn size
  "Returns the size (as a scalar tensor) of a dimension(s) of `nd`. `options` can contain at most one key,
  `:axis`, that is either an integer or a seq of integers. If multiple axes are specified, the returned value
  is the product of their sizes. If no axis is specified, the number of elements in the array is returned."
  ([nd]
   (size nd {}))
  ([nd options]
   (u/check-key-validity options [] [:axis])
   (let [axis (:axis options)]
     (if (not axis)
       (element-count nd)
       (size-axes nd (if (vector? axis)
                       (distinct axis)
                       [axis]))))))

(defop shape
  "Returns a clojure vector of `nd`'s shape (`nil` for a scalar). Non-backpropagating."
  [^:nd nd]
  :v (m/shape nd))

(defop reshape
  "Changes the shape of `nd` to `shape`. Throw an Exception if the number of elements in `nd` differs from
  the number of elements in a tensor of `shape`."
  [^:nd nd shape]
  :v (let [shape-vec shape]
       (when (not= (m/ecount nd) (apply * shape-vec))
         (u/throw-str "Could not reshape tensor of shape " (m/shape nd) " to shape " shape-vec "."))
       (m/reshape nd shape-vec))
  :dnd (m/reshape dv (m/shape nd)))

(defop- join-2-along
  "Concatenates tensors `a` and `b` along dimension `dim`."
  [dim ^:nd a ^:nd b]
  :v (do
       (when (not (valid-dim? dim))
         (u/throw-str "Cannot join along non-scalar dimension " dim ". Dimension must be a scalar."))
       (m/join-along dim a b))
  :da (m/submatrix dv dim [0 (m/dimension-count a dim)])
  :db (m/submatrix dv dim [(m/dimension-count a dim) (m/dimension-count b dim)]))

(defn join-along
  "Concatenates an arbitrary number of tensors in `nds` along `dim`."
  [dim & nds]
  (case (count nds)
    0 (u/throw-str "Attempt to join an empty list of tensors. `join-along` requires at least one tensor.")
    1 (first nds)
    (reduce #(join-2-along dim %1 %2) nds)))

(defop partition-along-op
  [^:nd nd axis partition-size step-size]
  :v (m/partition-along nd axis partition-size step-size)
  :dnd (let [result (m/zeros (m/shape nd))
             partitioned-axis-size (m/dimension-count nd axis)
             partition-starts (range 0 partitioned-axis-size step-size)
             partition-ends (concat (rest partition-starts)
                                    [partitioned-axis-size])
             partition-ranges (u/zip partition-starts partition-ends)]
         (->> (u/zip dv partition-ranges)
              u/unchunk
              (pmap (fn [[partition partition-range]]
                      (m/set-axis-range! result axis partition-range partition)))
              doall)
         result))

(defn partition-along
  ([nd]
   (partition-along nd 0))
  ([nd axis]
   (partition-along nd axis 1))
  ([nd axis partition-size]
   (partition-along nd axis partition-size partition-size))
  ([nd axis partition-size step-size]
   (partition-along-op nd axis partition-size step-size)))

(defop slices-op
  [^:nd nd axis]
  :v (m/slices nd axis)
  :dnd (apply m/stack axis dv))

(defn slices
  ([nd]
   (slices nd 0))
  ([nd axis]
   (slices-op nd axis)))

(defop- shift-all-by
  "Shifts the elements of tensor `nd` by the number of elements specified in `shifts`, a vector of shift
  amounts for each dimension. See `shift`."
  [^:nd nd shifts]
  :v (m/shift nd shifts)
  :dnd (m/shift dv (map - shifts)))

(defop- shift-dim-by
  "Shifts dimension `dim` of `nd` by `shift-amount` elements. See `shift`."
  [^:nd nd dim shift-amount]
  :v (m/shift nd dim shift-amount)
  :dnd (m/shift dv dim (- shift-amount)))

(defn shift
  "Shifts the elements in `nd` by `shift-amount` elements along single dimension `dim` or by the number of
  elements specified in `shifts`, a vector of shift amounts for the axes at that index. Positive shifts are
  up/left, and negative shifts are down/right. For instance, `(shift nd 1 3)` shifts the columns of a matrix
  left by three elements; and `(shift nd [-1, 2])` shifts the rows of `nd` down by 1 and the columns of nd
  left by 2 (the first row and last two columns will be all zeros). (`shifts`, `dim`, and `shift-amount` are
  non-backpropagating.)`"
  ([nd dim shift-amount]
   (shift-dim-by nd dim shift-amount))
  ([nd shifts]
   (shift-all-by nd shifts)))

(defop- conj-to-shape
  "Appends tensor or number `x` to clojure vector `shape`."
  [shape x]
  :v (conj (vec shape) (m/->int x)))

(defn make-shape
  "An op that, given any number of dimension `sizes` (either as scalar tensors or as numbers), returns a
  tensor shape (a vector of natural numbers)."
  [& sizes]
  (if-not (seq sizes)
    (const [])
    (reduce conj-to-shape [] sizes)))

(defop select-range
  "Returns a view of a (not necessarily continuous) selection of `nd`. `selections` is a seq of objects
  indicating what range of the corresponding dimension should be selected. These objects can be:

    - a number - indicating a slice of that dimension (the dimension will be eliminated)
    - a two-element vector like [start stop] - indicating a range of slices in [start, stop)
    - a three-element vector like [start stop step] - indicating a range of every `step`th slice in
      [start, stop)
    - `:first` - indicating the first slice (the dimension will be eliminated)
    - `:last` - indicating the last slice (the dimension will be eliminated)
    - `:all` - indicating that all slices through that dimension should be kept
    - `:butlast` - indicating that all but the last slice of that dimension should be kept
    - `:rest` - indicating that all but the first slice of that dimension should be kept

  The size of `selections` should match the dimensionality of `nd`."
  [^:nd nd selections]
  :v (apply m/select-range nd selections)
  :dnd (apply m/set-range! (m/zeros (m/shape nd))
              (concat selections [dv])))

(defn- get-paddings
  "Given the dimsionality of a tensor to pad and `padding`, an argument accepted by `pad-with`, returns
  a vector like [left-padding right-padding], where `left-padding` and `right-padding` are, respectively, the
  number of elements to add at the beginning and end of each dimension."
  [target-dimensionality padding]
  (cond (number? padding) (->> (int padding)
                               (repeat target-dimensionality)
                               (repeat 2))
        (and (seq padding) (number? (first padding))) (->> (mapv int padding)
                                                           (repeat 2))
        :else (partition (count padding) (apply interleave padding))))

(defn- get-unpadded-subtensor
  "Given tensor `padded-nd` that has been padded with `padding`, returns a submatrix corresponding to the
  unpadded region (i.e. the original tensor)."
  [padded-nd padding]
  (let [[left-padding right-padding] (get-paddings (m/dimensionality padded-nd) padding)]
    (->> (mapv (fn [l r size]
                 [l (- size r l)])
               left-padding right-padding (m/shape padded-nd))
         (m/submatrix padded-nd))))

(defop pad-with
  "Pads tensor `nd` with `padding` repetitions of `fill-value`. `padding` can be:

    - a scalar that is the number of elements to add on both sides of every dimension
    - a vector of size `(dimensionality nd)`, where each element is the amount of padding to add to both
      sides of each dimension
    - a matrix of size `[(dimensionality nd) 2]`, where each row is [left-padding right-padding] for that
      dimension

  `padding` is non-backpropagating."
  [^:nd nd ^:nd fill-value padding]
  :v (let [nd-dimensionality (m/dimensionality nd)
           nd-shape (m/shape nd)
           [left-padding right-padding] (get-paddings nd-dimensionality padding)
           total-padding (mapv + left-padding right-padding)
           _ (when-not (= (count total-padding) nd-dimensionality)
               (u/throw-str "Dimension mismatch. Cannot pad tensor of shape '"
                            nd-shape "' with paddings '" padding "'."))
           result-shape (mapv (comp int +) total-padding nd-shape)
           result (m/filled result-shape fill-value)]
       (-> (m/submatrix result (map vector left-padding nd-shape))
           (m/assign! nd))
       result)
  :dnd (get-unpadded-subtensor dv padding)
  ; TODO: Figure out if there's a more efficient way to do this (or if we should be backpropagating
  ; fill-value at all).
  :dfill-value (let [total-sum (m/esum dv)
                     unpadded-sum (m/esum (get-unpadded-subtensor dv padding))]
                 (m/sub total-sum unpadded-sum)))

(defn pad
  "Like `pad-with` but uses 0 as the `fill-value`."
  [^:nd nd padding]
  (pad-with nd 0 padding))

;;
;; Fill ops
;; These ops all take a shape vector and return a tensor of that shape filled with certain values. They are
;; all non-backpropagating.
;;

(defop zeros
  "Returns a tensor of `shape` filled with 0's. Non-backpropagating."
  [shape]
  :v (m/zeros shape))

(defop ones
  "Returns a tensor of `shape` filled with 1's. Non-backpropagating."
  [shape]
  :v (m/ones shape))

(defop random-uniform
  "Returns a tensor of `shape` filled numbers sampled randomly from [0, 1). Non-backpropagating."
  [shape]
  :v (m/sample-uniform shape))

(defop random-normal
  "Returns a tensor of `shape` filled with numbers sampled randomly from a normal distribution with mean 0
  and standard deviation 1. Non-backpropagating."
  [shape]
  :v (m/sample-normal shape))

;;
;; Element-wise logical and comparison ops
;; These differ from normal Clojure logical ops in that they never short circuit and always return `0` or `1`.
;; They're all non-backpropagating.
;;

;; TODO: Handle these in a cleaner, more efficient way. We shouldn't need to do arithmetic calculations and
;; return double arrays to do these operations.
(defop- eq-2
  "Elementwise a == b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/eq a b))

(defop- ne-2
  "Elementwise a != b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/ne a b))

; TODO: Use nd4j `and`.
(defop- and-2
  "Elementwise a && b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/array (m/eq (m/add (m/ne a (m/scalar-array 0)) (m/ne b (m/scalar-array 0))) (m/scalar 2))))

(defop- or-2
  "Elementwise a || b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/gt (m/add a b) (m/array 0)))

(defn tensor-and
  "Elementwise logical 'and' for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (const (m/array 1))
    1 (ne-2 (first operands) (m/array 0))
    (reduce and-2 operands)))

(defn tensor-or
  "Elementwise logical 'or' for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (const (m/array 0))
    1 (ne-2 (first operands) (m/array 0))
    (reduce or-2 operands)))

(defn eq
  "Elementwise logical equality for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `=`/`eq` with 0 operands. At least one is required.")
    1 (ones (shape (first operands)))
    2 (eq-2 (first operands) (second operands))
    (let [first-operand (first operands)]
      (->> (rest operands)
           (map #(eq-2 first-operand %))
           (apply tensor-and)))))

(defn ne
  "Elementwise logical inequality for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `not=`/`ne` with 0 operands. At least one is required.")
    1 (zeros (shape (first operands)))
    2 (ne-2 (first operands) (second operands))
    (let [first-operand (first operands)]
      (->> (rest operands)
           (map #(ne-2 first-operand %))
           (apply or-2)))))

(defop tensor-not
  "Elementwise logical 'not'. Non-backpropagating."
  [^:nd nd]
  :v (m/eq nd (m/scalar-array 0)))

(defop- gt-2
  "Elementwise a > b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/gt a b))

(defn gt
  "Elementwise > for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `gt`/`>` with 0 operands. At least one is required.")
    1 (ones (shape (first operands)))
    2 (apply gt-2 operands)
    (->> (partition 2 1 operands)
         (map #(apply gt-2 %))
         (apply tensor-and))))

(defop- ge-2
  "Elementwise a >= b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/ge a b))

(defn ge
  "Elementwise >= for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `ge`/`>=` with 0 operands. At least one is required.")
    1 (ones (shape (first operands)))
    2 (apply ge-2 operands)
    (->> (partition 2 1 operands)
         (map #(apply ge-2 %))
         (apply tensor-and))))

(defop- lt-2
  "Elementwise a < b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/lt a b))

(defn lt
  "Elementwise < for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `lt`/`<` with 0 operands. At least one is required.")
    1 (ones (shape (first operands)))
    2 (apply lt-2 operands)
    (->> (partition 2 1 operands)
         (map #(apply lt-2 %))
         (apply tensor-and))))

(defop- le-2
  "Elementwise a <= b. Non-backpropagating."
  [^:nd a ^:nd b]
  :v (m/le a b))

(defn le
  "Elementwise <= for an arbitrary number of operands. Non-backpropagating."
  [& operands]
  (case (count operands)
    0 (u/throw-str "Attempt to call `le`/`<=` with 0 operands. At least one is required.")
    1 (ones (shape (first operands)))
    2 (apply le-2 operands)
    (->> (partition 2 1 operands)
         (map #(apply le-2 %))
         (apply tensor-and))))

;;
;; Trigonometric and hyperbolic ops
;;

(defop cos
  "cos(n)"
  [^:nd n]
  :v (m/cos n)
  :dn (m/mul dv (m/negate (m/sin n))))

(defop sin
  "sin(n)"
  [^:nd n]
  :v (m/sin n)
  :dn (m/mul dv (m/cos n)))

(defop tan
  "tan(n)"
  [^:nd n]
  :v (m/tan n)
  :dn (m/div dv (m/pow (m/cos n) (m/array 2))))

(defop acos
  "cos-1(n)"
  [^:nd n]
  :v (m/acos n)
  :dn (m/div (m/negate dv) (m/sqrt (m/sub (m/array 1) (m/pow n (m/array 2))))))

(defop asin
  "sin-1(n)"
  [^:nd n]
  :v (m/asin n)
  :dn (m/div dv (m/sqrt (m/sub (m/array 1) (m/pow n (m/array 2))))))

(defop atan
  "tan-1(n)"
  [^:nd n]
  :v (m/atan n)
  :dn (m/div dv (m/add (m/array 1) (m/pow n (m/array 2)))))

(defop cosh
  "cosh(n)"
  [^:nd n]
  :v (m/cosh n)
  :dn (m/mul dv (m/sinh n)))

(defop sinh
  "sinh(n)"
  [^:nd n]
  :v (m/sinh n)
  :dn (m/mul dv (m/cosh n)))

(defop tanh
  "tanh(n)"
  [^:nd n]
  :v (m/tanh n)
  :dn (m/div dv (m/pow (m/cosh n) (m/array 2))))

;; Misc ops

(defn report
  "An op that accepts operands like:

     `(report o1 o2 o3 ... oi reporter-fn logger-fn)`

  where `o1`, `o2`, `o3`, ... `oi` are computation graph nodes. If `v1`, `v2`, `v3`, ..., `vi` are the
  values of nodes `o1`, `o2`, `o3`, ... `oi`, then `(reporter-fn v1 v2 v3 ... vi)` must return a single
  object, `reporter-value`. `(logger-fn reporter-value)` is called during forward propagation, purely for
  side effects. The value of the `report` node itself is `v1` (i.e. it just passes through the first
  operand."
  [& operands]
  (let [input-nodes (map #(construct-operand % false) (drop-last 2 operands))
        [reporter-fn logger-fn] (take-last 2 operands)]
    (when-not (and (fn? reporter-fn)
                   (fn? logger-fn))
      (u/throw-str "Invalid arguments to `report`. Last two arguments must be a reporter function and a "
                   "logger function, respectively."))
    (make-node
      {:type :op
       :op "report"
       :operands input-nodes
       :forward-fn (fn [& vals]
                     (do (logger-fn (apply reporter-fn vals))
                         (first vals)))
       :backward-fns (vector (fn [dy & vals] dy))})))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Core computation graph functions
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(declare forward-recursive backward)

(defn- add-grads
  "Returns the sum of gradients `a` and `b` using the appropriate operation for their types. Throws an
  exception if they are different types."
  [a b]
  (cond (and (m/array? a) (m/array? b)) (m/add a b)
        (and (sequential? a) (sequential? b)) (map add-grads a b)
        :else (u/throw-str "Cannot add gradients of type '" (type a) "' and type '" (type b) "'.")))

(defn- generate-zero-gradient
  "Given a node value, returns the 'zero gradient' for that node."
  [node-value]
  (cond (m/array? node-value) (-> node-value m/shape m/zeros)
        (sequential? node-value) (map generate-zero-gradient node-value)
        :else (m/array 0)))

(defn- generate-initial-gradient
  "Given a node value, returns the 'ones gradient' or 'initial gradient' for that node (i.e. the gradient
  for that node were it root of a graph)."
  [node-value]
  (cond (m/array? node-value) (-> node-value m/shape m/ones)
        (sequential? node-value) (map generate-initial-gradient node-value)
        :else (u/throw-str "Cannot generate initial gradient for node value of type '"
                           (type node-value) "'.")))

; TODO: Clean up the `fork` implmenetation (or get rid of `fork` entirely in favor of some other abstraction
; for recurrent networks). The `fork` implementation encompasses the next several functions. Currently it's a
; disaster.

; TODO: Make a more formal data structure for the input map, so we don't have input maps with node names
; interspersed with these iteration data keys.
(defn- get-iteration-data-key
  "Given a fork node, return the name of the key to use in the forward propagation input map for the data
  within a fork node over time."
  [fork-node]
  (-> (if (keyword? fork-node)
        (str ":" (name fork-node))
        (get-node-name fork-node))
      (str "_iteration_data")
      keyword))

; TODO: Here and above, only dissoc dependencies when the value of the node actually changes?
(defn- forward-through-let-bindings
  "Returns an input-map with inputs in `bindings` bound to their values, given:
    - `bindings` - a seq of seqs like [binding-input-node binding-value-node]. `binding-input-node` is an
      input node whose value will be the value of `binding-value-node`
    - `input-map` - the input-map up to this point of forwrad propagation
    - `dependency-map` - a map of node names to nodes whose values will change when the key node's value
       changes; this allow recalculation of any values that should be updated with a new binding
    - `bind-seq?` - a boolean indicating if the binding values should be the complete value of the
      `binding-value-node`s (`false`) or if `binding-value-node` is a seq and the binding should be to the
       first value in the seq (`true`)"
  [bindings input-map dependency-map bind-seq?]
  (loop [bindings bindings
         cumulative-input-map input-map
         complete-inputs {}
         all-input-maps []]
    (if-let [binding (first bindings)]
      (let [[binding-input-node binding-value-branch] binding
            propped-vals (forward-recursive binding-value-branch cumulative-input-map)
            value-node-name (get-node-name binding-value-branch)
            input-value (get propped-vals value-node-name)
            binding-name (get-node-name binding-input-node)
            dependents (get dependency-map binding-name)]
        (recur (rest bindings)
               (-> (apply dissoc propped-vals dependents)
                   (assoc binding-name (if bind-seq?
                                         (first input-value)
                                         input-value)))
               (assoc complete-inputs binding-name input-value)
               (conj all-input-maps propped-vals)))
      [(merge cumulative-input-map
              complete-inputs) all-input-maps])))

(defn- fork-forward
  [node input-map]
  (let [{:keys [target-node-initial-value
                input-node-value-branch
                output-node
                recur-branches
                output-time-axis
                dynamic-node-names
                recur-target-names
                dynamic-node->dependents]} (get-node-data node)
        input-map-with-output-time-axis (forward-recursive output-time-axis input-map)
        [input-map-with-inputs-bound
         input-bindings-input-maps] (forward-through-let-bindings
                                      input-node-value-branch
                                      input-map-with-output-time-axis
                                      dynamic-node->dependents
                                      true)
        input-binding->values (->> input-node-value-branch
                                   (map (fn [[input-target-node _]]
                                          (let [target-name (get-node-name input-target-node)]
                                            [target-name (get input-map-with-inputs-bound target-name)])))
                                   (into {}))
        input-binding->timepoint-count (u/update-vals input-binding->values count)
        _ (when (-> input-binding->timepoint-count vals distinct count (> 1))
            (u/throw-str "Invalid input values to `fork`. Inputs must all have the same number of time points."
                         "Instead received: '" input-binding->timepoint-count "'."))
        inputs-over-time (->> (map (fn [[input-name values-over-time]]
                                     (map #(vector input-name %) values-over-time))
                                   input-binding->values)
                              (apply interleave)
                              (partition (count input-binding->values))
                              (map #(into {} %)))
        recur-branch-names (map get-node-name recur-branches)
        [initial-input-map
         recur-bindings-input-maps] (forward-through-let-bindings
                                      target-node-initial-value
                                      (merge input-map-with-inputs-bound (first inputs-over-time))
                                      dynamic-node->dependents
                                      false)
        recurred-values (select-keys initial-input-map recur-target-names)
        output-branch-name (get-node-name output-node)
        output-time-axis (m/->int (get input-map-with-inputs-bound (get-node-name output-time-axis)))
        [vals-over-time final-values]
          (loop [remaining-inputs inputs-over-time
                 vals-over-time []
                 recurred-values {}]
            (if-not (seq remaining-inputs)
              [vals-over-time (merge (last vals-over-time) recurred-values)]
              (let [changed-dependencies (if (seq vals-over-time)
                                           dynamic-node-names
                                           (->> target-node-initial-value
                                                last
                                                first
                                                get-node-name
                                                (get dynamic-node->dependents)))
                    timepoint-input-map (-> (or (last vals-over-time)
                                                initial-input-map)
                                            (merge (first remaining-inputs)
                                                   recurred-values)
                                            (#(apply dissoc % changed-dependencies)))
                    output-values (forward-recursive output-node timepoint-input-map)
                    recur-branch-prop-maps (reduce (fn [input-maps recur-branch]
                                                     ; We use the output map from propagation through the
                                                     ; previous node to avoid recomputing any common nodes
                                                     ; between branches, but we merge with recurred values to
                                                     ; ensure all recurrent branches are evaluated with the
                                                     ; values of all recurrent branches on the present iteration.
                                                     (conj input-maps
                                                           (forward-recursive recur-branch (merge (last input-maps)
                                                                                                  recurred-values))))
                                                   [output-values]
                                                   recur-branches)
                    new-recurred-values (->> (rest recur-branch-prop-maps)
                                             (u/zip recur-target-names recur-branch-names)
                                             (map (fn [[target-name branch-name prop-map]]
                                                    [target-name (get prop-map branch-name)]))
                                             (into {}))
                    complete-timepoint-input-map (merge (last recur-branch-prop-maps)
                                                        recurred-values)]
                (recur (rest remaining-inputs)
                       (conj vals-over-time complete-timepoint-input-map)
                       new-recurred-values))))
        fork-node-value (->> vals-over-time
                             (map #(get % output-branch-name))
                             (apply m/stack output-time-axis))]
    (when-not fork-node-value
      (u/throw-str "`fork` node did not produce any output."))
    (assoc final-values
      (get-node-name node) fork-node-value
      (get-iteration-data-key node) {:vals-over-time vals-over-time
                                     :input-bindings-input-maps input-bindings-input-maps
                                     :recur-bindings-input-maps recur-bindings-input-maps})))

(defn- fork-backward
  [node forward-values grad input-set propagated-recurred-grads]
  (let [{:keys [target-node-initial-value
                input-node-value-branch
                output-node
                recur-branches
                output-time-axis
                dynamic-node-names
                recur-target-names
                all-recur-target-names
                binding-level->recurred]} (get-node-data node)
        {:keys [vals-over-time
                input-bindings-input-maps
                recur-bindings-input-maps]} (->> node get-iteration-data-key (get forward-values))
        output-time-axis (m/->int (get forward-values (get-node-name output-time-axis)))
        split-grad (reverse (m/slices grad output-time-axis))
        input-target-names (map (comp get-node-name first) input-node-value-branch)
        desired-grads-set (into input-set (concat recur-target-names input-target-names))
        recur-target-name-branch (u/zip recur-target-names recur-branches)
        ; TODO: Also exclude immediate backpropagation of nodes that don't change in the output
        ; or recur branches. These can be backpropped once.
        boundary-nodes (->> (concat target-node-initial-value input-node-value-branch)
                            (map first))]
    (loop [remaining-timepoint-inputs (reverse vals-over-time)
           remaining-output-grads split-grad
           grads-over-time []
           recurred-grads propagated-recurred-grads]
      (if (seq remaining-timepoint-inputs)
        (let [timepoint-input-map (first remaining-timepoint-inputs)
              output-grad (first remaining-output-grads)
              last-timepoint? (-> remaining-timepoint-inputs rest seq not)
              branch-grads (->> (map (fn [[target-name recur-branch]]
                                       (when-let [recurred-grad (get recurred-grads target-name)]
                                         {(get-node-name recur-branch) recurred-grad}))
                                     recur-target-name-branch)
                                (apply merge-with add-grads {(get-node-name output-node) output-grad}))
              recurred (if last-timepoint?
                         (get binding-level->recurred ::first-output)
                         (get binding-level->recurred ::output))
              new-recurred-grads (backward (cons output-node
                                                 (keep (fn [[target-name recur-branch]]
                                                         (when (get recurred-grads target-name)
                                                           recur-branch))
                                                       recur-target-name-branch))
                                           timepoint-input-map
                                           desired-grads-set
                                           branch-grads
                                           {:stop-at boundary-nodes
                                            :return-all false
                                            :recurred-grads (select-keys recurred-grads recurred)})
              [recurred-through-recur-initializer
               recur-initializer-grads]
                (when last-timepoint?
                  (loop [bindings (reverse target-node-initial-value)
                         input-maps (reverse recur-bindings-input-maps)
                         recurred-through-recur-initializer {}
                         recur-initializer-grads {}
                         ; TODO: Why not `new-recurred-grads` here?
                         recurred-from-prev-level recurred-grads]
                    (if-not (seq bindings)
                      [recurred-through-recur-initializer recur-initializer-grads]
                      (let [[target-node value-branch] (first bindings)
                            input-map (first input-maps)
                            target-node-name (get-node-name target-node)
                            value-branch-name (get-node-name value-branch)
                            recurred-grad-names (get binding-level->recurred
                                                     target-node-name)
                            grads (backward value-branch
                                            input-map
                                            desired-grads-set
                                            (get new-recurred-grads target-node-name
                                                 (generate-zero-gradient (get input-map
                                                                              value-branch-name)))
                                            {:return-all false
                                             :recurred-grads (select-keys recurred-from-prev-level
                                                                          recurred-grad-names)})
                            recurred (select-keys grads recurred-grad-names)
                            non-recurred (apply dissoc grads recurred-grad-names)]
                        (recur (rest bindings)
                               (rest input-maps)
                               (merge-with m/add recurred-through-recur-initializer recurred)
                               (merge-with m/add recur-initializer-grads non-recurred)
                               (merge recurred-from-prev-level recurred))))))
              ; TODO: Only do this if we need to (i.e. if the gradient of an upstream node is requested)
              all-recurred (concat recurred recur-target-names)
              rt (merge new-recurred-grads recurred-through-recur-initializer)
              next-recurred (merge rt recur-initializer-grads)
              new-grads-over-time (conj grads-over-time (if last-timepoint?
                                                          (merge (merge-with add-grads
                                                                             (apply dissoc rt all-recurred)
                                                                             recur-initializer-grads)
                                                                 (select-keys next-recurred all-recurred))
                                                          (apply dissoc next-recurred all-recurred)))
              input-grads
                (when last-timepoint?
                  (loop [bindings (reverse input-node-value-branch)
                         input-maps (reverse input-bindings-input-maps)
                         recurred-through-initializer {}
                         input-initializer-grads {}
                         recurred-from-prev-level (merge recurred-grads recurred-through-recur-initializer)]
                    (if-not (seq bindings)
                      input-initializer-grads
                      (let [[target-node value-branch] (first bindings)
                            input-map (first input-maps)
                            target-node-name (get-node-name target-node)
                            value-branch-name (get-node-name value-branch)
                            recurred-grad-names (get binding-level->recurred
                                                     target-node-name)
                            grads (->> new-grads-over-time
                                       reverse
                                       (map #(get % target-node-name
                                                  (-> recur-bindings-input-maps
                                                      first
                                                      (get target-node-name)
                                                      generate-zero-gradient)))
                                       (#(backward value-branch
                                                   input-map
                                                   desired-grads-set
                                                   %
                                                   {:return-all false
                                                    :recurred-grads (select-keys recurred-from-prev-level
                                                                                 recurred-grad-names)})))
                            ; TODO: Clean this up (can just use grads and get rid of recurred/non-recurred-grads)
                            recurred (select-keys grads recurred-grad-names)
                            non-recurred grads #_(apply dissoc grads recurred-grad-names)]
                        (recur (rest bindings)
                               (rest input-maps)
                               (merge-with m/add recurred-through-initializer recurred)
                               (merge-with m/add input-initializer-grads non-recurred)
                               (merge recurred-from-prev-level recurred))))))]
          (recur (rest remaining-timepoint-inputs)
                 (rest remaining-output-grads)
                 (if last-timepoint?
                   (conj new-grads-over-time input-grads)
                   new-grads-over-time)
                 next-recurred))
        {:grads (apply merge-with add-grads grads-over-time)
         ; TODO: Only include nodes if necessary
         :nodes []}))))

(defn fork
  "Given:

    - `input-bindings` - a vector with alternating input and seq-valued nodes of all the same length, e.g.
      `[:input-a (slices :a) :input-b (slices :b)... ]`.
    - `recur-bindings` - a vector with alternating input and initialization value nodes, e.g.
      `[:recur-target-a :initialization-value :recur-target-b :initialization-value-b ...]`
    - `output-node` - a graph node
    - `recur-branches` - a seq of nodes equal in number to the number of recurrence targets specified in
      `recur-bindings`
    - `options` - a map of optional key/value pairs:
        - `output-time-axis` (deafult 0) - axis along which to concatenate output values (see below).

  this evaluates `output-node` exactly n times, where n is the count of elements in the seqs in
  `input-bindings`. The value of the `fork` node will be a tensor produced by concatenating together the n
  values of `output-node` along `:output-time-axis`. The nth output value is the result of evaluating
  `output-node` in a context where each input node in `input-bindings` is bound to the nth value in its
  corresponding seq and each input node in `recur-bindings` is bound to the value of the corresponding node
  in `recur-branches` on the (n-1)th iteration, or to the value of the corresponding ndoe in `recur-bindings`
  when n=0. None of the input nodes used for bindings may be used elsewhere in the graph outside this `fork`
  node."
  ([input-bindings recur-bindings output-node recur-branches]
   (fork input-bindings recur-bindings output-node recur-branches {}))
  ([input-bindings recur-bindings output-node recur-branches options]
   (u/check-key-validity options [] [:output-time-axis])
   (when-not (and (vector? recur-bindings) (even? (count recur-bindings)))
     (u/throw-str "Invalid arguments to `fork`. Recurred node bindings must be a vector with an even number of "
                  "forms."))
   (when-not (and (vector? input-bindings) (even? (count input-bindings)))
     (u/throw-str "Invalid arguments to `fork`. Input node bindings must be a vector with an even number of "
                  "forms."))
   (let [recur-nodes (map construct-operand recur-bindings)
         recur-node-bindings (partition 2 recur-nodes)
         recur-target-names (mapv (comp get-node-name first) recur-node-bindings)
         recur-branch-nodes (map construct-operand recur-branches)
         _ (when (not= (count recur-branches) (count recur-node-bindings))
             (u/throw-str "Number of recurrent branches must equal number of initialzed recurrence targets "
                          "for `fork` with recurrence targets: " (->> recur-node-bindings
                                                                      (mapv (comp get-node-name first)))))
         input-node-bindings (->> (partition 2 input-bindings)
                                  (map (fn [[binding-node value-node]]
                                         [(construct-operand binding-node)
                                          (construct-operand value-node false)])))
         input-node-names (mapv (comp get-node-name first) input-node-bindings)
         all-locally-bound-names (concat recur-target-names input-node-names)
         _ (when-let [repeated-targets (u/indistinct all-locally-bound-names)]
             (u/throw-str "Invalid network structure: multiple branches are bound to targets: '"
                          (vec repeated-targets) "'."))
         _ (when-let [[node-type node-name] (->> (concat recur-node-bindings input-node-bindings)
                                                 (keep #(when-not (is-input? (first %))
                                                          ((juxt get-node-type get-node-name) (first %))))
                                                 first)]
             (u/throw-str "Cannot bind values to a non-input node. Trying to bind to '" node-name "', but "
                          "it is of type '" node-type "'."))
         output-branch (construct-operand output-node)
         dynamic-input-names (set all-locally-bound-names)
         ; TODO: Streamline this - we're looping through all the subbranch nodes several times.
         all-contained-nodes (->> (concat recur-branch-nodes
                                          [output-branch]
                                          (map second input-node-bindings)
                                          (map second recur-node-bindings))
                                  (mapcat get-all-graph-nodes))
         all-recur-target-names (->> all-contained-nodes
                                     (mapcat (comp :recur-target-names get-node-data))
                                     (concat recur-target-names)
                                     set)
         dynamic-node->dependents (->> all-contained-nodes
                                       (map (fn [node]
                                              (let [node-name (get-node-name node)]
                                                (->> (get-upstream-nodes node)
                                                     (set/intersection dynamic-input-names)
                                                     (map #(vector % #{node-name}))
                                                     (into {})))))
                                       (apply merge-with set/union))
         all-output-level-branches (concat [output-branch])
         binding-level->recurred
           (loop [bindings (->> (concat input-node-bindings
                                        recur-node-bindings)
                                (map (fn [[binding-node value-node]]
                                       [(get-node-name binding-node) [value-node]]))
                                (#(concat % [[::first-output (cons output-branch recur-branch-nodes)]
                                             [::output (cons output-branch recur-branch-nodes)]])))
                  needs-evaluation #{}
                  binding-level->recurred {}]
             ; TODO: Clean this up
             (if-not (seq bindings)
               binding-level->recurred
               (let [[level-name value-branches] (first bindings)
                     evaluated (mapcat #(filter-graph-nodes
                                          (comp needs-evaluation get-node-name)
                                          %)
                                       value-branches)
                     evaluated-recur-targets (mapcat (comp :recur-target-names get-node-data)
                                                     evaluated)
                     all-evaluated (set (mapcat get-upstream-nodes evaluated))
                     now-needs-evaluation (set/union
                                            (set/difference needs-evaluation
                                                            (set (map get-node-name value-branches))
                                                            all-evaluated)
                                            (if (= level-name ::first-output)
                                              (apply set/union (vals dynamic-node->dependents))
                                              (dynamic-node->dependents level-name)))]
                 (recur (rest bindings)
                        now-needs-evaluation
                        (assoc binding-level->recurred level-name (set evaluated-recur-targets))))))
         dynamic-node-names (->> dynamic-node->dependents
                                 vals
                                 (apply concat)
                                 set)]
     (make-node
       {:type :op
        :op "fork"
        :operands (concat (apply concat input-node-bindings)
                          [output-branch]
                          recur-nodes
                          recur-branch-nodes)
        :data {:target-node-initial-value recur-node-bindings
               :input-node-value-branch input-node-bindings
               :output-node output-branch
               :recur-branches recur-branch-nodes
               :output-time-axis (construct-operand (or (:output-time-axis options)
                                                        0))
               :dynamic-node->dependents dynamic-node->dependents
               ; TODO: Remove if no longer used.
               :dynamic-node-names dynamic-node-names
               :recur-target-names recur-target-names
               :all-recur-target-names all-recur-target-names
               :binding-level->recurred binding-level->recurred}
        :private-nodes dynamic-input-names
        :forward-fn fork-forward
        :backward-fns [fork-backward]}))))

;; Evaluation fns
(declare raise-runtime-exception)
(defn- forward-recursive
  ([node input-map]
   (try
     (let [node-name (get-node-name node)]
       (cond (input-map node-name) input-map
             (is-input? node) (let [initializer (get-node-initializer node)]
                                #_(println "Initializing " node-name)
                                (if (not initializer)
                                  (u/throw-str "Unable to find value for input " node-name
                                               " in input-map with inputs '" (keys input-map) "'.")
                                  (assoc input-map node-name (initializer input-map))))
             (is-const? node) (assoc input-map node-name (get-node-value node))
             (= "tensor-if" (get-node-op node)) (let [[flag if-true if-false] (get-node-operands node)
                                                      input-map-with-flag (forward-recursive flag input-map)
                                                      flag-value (get input-map-with-flag (get-node-name flag))
                                                      next-node (if (m/equals flag-value (m/array 0)) if-false if-true)
                                                      propagated-input-map (forward-recursive next-node input-map-with-flag)
                                                      ; The value of the "if" node will be the same as the value of the
                                                      ; node to which flow went.
                                                      if-node-value (get propagated-input-map (get-node-name next-node))]
                                                  (assoc propagated-input-map node-name if-node-value))
             (= "fork" (get-node-op node)) ((get-node-forward-fn node) node input-map)
             :else (let [operands (get-node-operands node)
                         ; Run forward on nodes with initializers after nodes without initializers in case
                         ; initialization depends on values of any of the other operands.
                         updated-input-map (->> (sort-by (comp boolean get-node-initializer) operands)
                                                (reduce (fn [input-map operand]
                                                          (forward-recursive operand input-map))
                                                        input-map))
                         operand-values (map (comp updated-input-map get-node-name) operands)
                         forward-fn (get-node-forward-fn node)
                         this-node-value (try (apply forward-fn operand-values)
                                              (catch Throwable e
                                                (raise-runtime-exception node "evaluating" e operand-values nil)))]
                     (assoc updated-input-map node-name this-node-value))))
     (catch Throwable e
       (raise-runtime-exception node "evaluating" e)))))

(defn forward
  "Given a computation graph `node` and a map of input variable names to their tensor values, runs the
  computation and returns a map of node to value."
  ([node]
   (forward-recursive node {}))
  ([node input-map]
   (validate-graph node)
   (forward-recursive (construct-operand node) (prepare-input-map node input-map))))

(defn- back-one
  "Given gradient `grad` for node `node`, performs one step of backpropagation from `node`, and returns a map
  like { :nodes :grads } where `:nodes` is a seq of operand nodes and `:grads` is a map from node name to
  gradient. `inputs` is the set of node names for which we want gradients. `recurred-grads` is a map of
  node names to gradient values for nodes that are being propagated through a recurrent network."
  [node values grad input-set recurred-grads]
  (try
    (cond (not (is-op? node)) {:nodes [] :grads {}}
          (= "tensor-if" (get-node-op node)) (let [[flag if-true if-false] (get-node-operands node)
                                                   flag-value (get values (get-node-name flag))
                                                   next-node (if (m/equals flag-value (m/array 0)) if-false if-true)]
                                               (if-not (or (input-set (get-node-name next-node))
                                                           (seq (set/intersection input-set
                                                                                  (get-upstream-nodes next-node))))
                                                 {:nodes [] :grads {}}
                                                 next-node)
                                               {:nodes [next-node] :grads {(get-node-name next-node) grad}})
          (= "fork" (get-node-op node)) ((first (get-node-backward-fns node)) node values grad input-set recurred-grads)
          :else (let [all-operand-nodes (get-node-operands node)
                      all-operand-values (map (fn [operand-node]
                                                (if (contains? values (get-node-name operand-node))
                                                  (get values (get-node-name operand-node))
                                                  (u/throw-str "Error computing gradient during backpropagation. Could not "
                                                               "find value for node " (get-node-name operand-node) ".")))
                                              all-operand-nodes)]
                  (try
                    (reduce (fn [result [operand-node back-fn]]
                              (let [node-name (get-node-name operand-node)
                                    upstream-node-names (get-upstream-nodes operand-node)]
                                (if-not (and (or (input-set node-name)
                                                 (seq (set/intersection input-set upstream-node-names)))
                                             back-fn)
                                  result
                                  (-> (update result :nodes conj operand-node)
                                      (update :grads (partial merge-with add-grads {node-name (apply back-fn grad all-operand-values)}))))))
                            {:nodes [] :grads {}}
                            (u/zip all-operand-nodes
                                   (get-node-backward-fns node)))
                    (catch Throwable e
                      (raise-runtime-exception node "backpropagating through" e all-operand-values grad)))))
    (catch Throwable e
      (raise-runtime-exception node "backpropagating through" e nil grad))))

(defn backward
  "Given:

    - `node` - a computation graph node
    - `values` - a map of input values (variable name -> tensor)
    - `inputs` - a seq of names of inputs for which to return gradients; if `nil`, all gradients are returned
    - `grad` - the propagated gradient tensor
    - `options` - a map that can contain the following keys:
      - `:stop-at` - a seq of nodes at which to stop backpropagation. This can be useful (e.g. during
        backprop through recurrent networks) for accumulating the gradient of a node outside of `backward`
        before backpropagating through it.
      - `:return-all` - boolean indicating what gradient values to return for nodes whose names are in
        `inputs` but that weren't encountered during backpropagation. If `true`, a zero gradient is returned;
        otherwise, the node name is omitted from the returned map.
      - `:recurred-grads` - a map of node name to gradients being propagated through a recurrent network.

  this returns a map of gradients for each input. `values` may be omitted only if the graph has no variables.
  If `grad` is omitted, a tensor of the same shape as the node value filled with all 1's will be
  assumed."
  ([node]
   (backward node {}))
  ([node values]
   (backward node values nil))
  ([node values inputs]
   (backward node values inputs nil))
  ([node values inputs grad]
   (backward node values inputs grad {}))
  ([node values inputs grad options]
   ; TODO: Clean this up
   (if-not (sequential? node)
     (backward
       [node]
       values
       inputs
       (let [node-name (get-node-name node)]
         {node-name (or grad
                        (->> (get values (get-node-name node))
                             generate-initial-gradient))})
       options)
     (do
       (u/check-key-validity options [] [:stop-at :return-all :recurred-grads])
       (let [input-set (->> (or inputs
                                (->> (mapcat get-all-graph-nodes node)
                                     (mapv get-node-name)))
                            (into #{}))
             stop-at-set (set (:stop-at options))
             recurred-grads (:recurred-grads options)
             node-name (get-node-name node)]
         ; TODO: Clean up argument names
         (loop [nodes (distinct node)
                grads grad]
           (if-not (seq nodes)
             (let [found (select-keys grads input-set)
                   missing (if (get options :return-all true)
                             (->> (set (keys found))
                                  (set/difference input-set)
                                  (mapv (fn [input-key]
                                          [input-key (->> (get values input-key)
                                                          generate-zero-gradient)]))
                                  (into {}))
                             {})]
               (merge found
                      missing))
             (let [all-upstream-nodes (->> (mapcat get-upstream-nodes nodes)
                                           (into #{}))
                   ; We can do backprop from a node only once it is no longer upstream from any other nodes
                   ; that need to be expanded.
                   ready?->nodes (group-by #(-> % get-node-name all-upstream-nodes not) nodes)
                   ready-for-backprop (get ready?->nodes true)
                   not-ready (get ready?->nodes false)
                   back-results (mapv #(let [node-name (get-node-name %)
                                             node-grad (get grads node-name)]
                                         (back-one % values node-grad input-set recurred-grads))
                                      ready-for-backprop)
                   new-nodes (->> (mapv :nodes back-results)
                                  (apply concat not-ready)
                                  distinct
                                  (filter (comp not stop-at-set)))
                   new-grads (->> (mapv :grads back-results)
                                  (apply merge-with add-grads grads))]
               (when (= new-nodes nodes)
                 (u/throw-str "Unable to complete backpropagation. Target node is unreachable or graph is "
                              "malformed."))
               (recur new-nodes
                      new-grads)))))))))

(defn evaluate
  "Given a computation graph `node` and a map of input variable names to their tensor values, runs the
  computation and returns the value of the final result."
  ([node]
   (evaluate node {}))
  ([node input-map]
   (get (forward node input-map)
        (get-node-name node))))

(defn get-gradients
  "Given a computation graph `node` and a map of input variable names to their tensor values, runs forward
  and back propagation and returns a map of all gradients."
  ([node]
   (get-gradients node {}))
  ([node input-map]
   (get-gradients node input-map {:return-all false}))
  ([node input-map options]
   (backward node
             (forward node input-map)
             nil
             nil
             options)))

(def- sym->op
  ; Operations on Tensure tensors
  {'+ add
   '* mul
   '- sub
   '/ div
   'esum esum
   'mmul mmul
   'pow pow
   'log log
   'exp exp
   'max max
   'min min
   'max-along max-along
   'min-along min-along
   'max-mask max-mask
   'min-mask min-mask
   'abs abs
   'negate negate
   'sum-along sum-along
   'transpose transpose
   'size size
   'zeros zeros
   'ones ones
   'random-uniform random-uniform
   'random-normal random-normal
   'and tensor-and
   'or tensor-or
   'not tensor-not
   '= eq
   'not= ne
   '> gt
   '>= ge
   '< lt
   '<= le
   'cos cos
   'sin sin
   'tan tan
   'acos acos
   'asin asin
   'atan atan
   'cosh cosh
   'sinh sinh
   'tanh tanh
   'reshape reshape
   'join-along join-along
   'partition-along partition-along
   'slices slices
   'shift shift
   'select-range select-range
   'pad-with pad-with
   'pad pad
   'report report
   ; Special operations (handled specially by `forward` and `backward`.
   'tensor-if tensor-if
   'fork fork
   ; Shape operations - these return clojure vectors rather than tensors.
   'shape shape
   'make-shape make-shape
   ; Misc
   'named named})

; TODO: Refactor `G` so this can be private.
(defn get-calling-line-number
  "Given a callstack (seq of `StackTraceElements`) leading to creation of a node via an op in a `G` form
  (see `G` below), returns (if possible) a proposed line number of the original op call. This is fragile.
  It depends on implementation details of Clojure as well as ranvier and is thus liable to break in the
  future. However, having these line numbers can greatly simplify debugging."
  [callstack]
  (-?>> (keep #(when (= (.getMethodName %) "invokeStatic")
                 %)
              callstack)
        first
        (#(.getLineNumber %))))

(defmacro G
  "Receives a single form and returns a computation graph. This macro provides two advantages over
  constructing graphs by just calling ops directly:

    1. Any symbol in `body` that matches the alias of an op will resolve to that op. For instance, inside of
       `G`, `*` will resolve to the ranvier `mul` op, rather than to `clojure.core/*`.
    2. The nodes returned by ops in the body of `G` receive additional metadata that can greatly facilitate
       debugging. Runtime exceptions from graphs constructed with `G` will in many cases contain information
       about where the node was constructed; such information is not available in exceptions arising from
       evaluation of graph nodes constructed outside of `G`."
  [body]
  (let [creation-metadata {:form &form
                           :g-form-line-num (:line (meta &form))
                           :ns *ns*
                           :src-file-path *file*}
        op-bindings (vec (mapcat
                           (fn [[sym op]]
                             [sym
                              `(fn [& args#]
                                 (let [result-node# (apply ~op args#)]
                                   (with-meta result-node#
                                     (assoc (meta result-node#)
                                       ::created (assoc '~creation-metadata
                                                   :calling-line-num ~'(ranvier.core/get-calling-line-number
                                                                         (.getStackTrace (Thread/currentThread))))))))])
                           sym->op))]
    `(let ~op-bindings
       (~construct-operand ~body))))

(defn- is-ranvier-runtime-exception?
  "Returns true iff `e` is an `ExceptionInfo` thrown by `raise-runtime-exception` below."
  [e]
  (-> (ex-data e)
      ::ranvier-runtime-exception
      boolean))

(defn- raise-runtime-exception
  "Given:

    - `node` - A ranvier computation graph node that led to some type of caught 'runtime' exception
    - `verb` - A string describing the action that produced the exception (e.g. 'evaluating' or 'backpropagating')
    - `e` - The caught exception
    - `operand-values` (default `nil`) - A seq of values of `node`'s operands, if applicable
    - `grad` (default `nil`) - `node`'s gradient during back-propagation, if applicable

  this throws an `ExceptionInfo` with information extracted and derived form `node`'s metadata, if available.
  If `e` was produced by this function, this just rethrows it."
  ([node verb e]
   (raise-runtime-exception node verb e nil nil))
  ([node verb e operand-values grad]
   (if (is-ranvier-runtime-exception? e)
     (throw e)
     (let [flanking-lines 1 ; Number of lines in src before and after exception line to show.
           {:keys [g-form-line-num
                   src-file-path
                   calling-line-num]
            :as creation-info} (::created (meta node))
           ; If we were able to parse the calling line from the stack trace, use that. Otherwise, use the line
           ; of the containing G-form.
           src-line (or calling-line-num g-form-line-num)
           src-text (when (and src-line src-file-path)
                      (try
                        (let [starting-line (- src-line flanking-lines)
                              lines (with-open [rdr (io/reader src-file-path)]
                                      (->> (line-seq rdr)
                                           (drop (- src-line flanking-lines 1))
                                           (take (inc (* 2 flanking-lines)))
                                           (map-indexed (fn [i line]
                                                          (let [line-num (+ starting-line i)
                                                                leading-char (if (= line-num src-line)
                                                                               "►"
                                                                               " ")]
                                                            (str "\t" leading-char " " line-num "   " line))))
                                           ; CIDER/emacs won't print single newlines in Exception messages,
                                           ; so the double-newline is to compensate for the quirk of that
                                           ; tooling specifically.
                                           (string/join "\n\n")))]
                          lines)
                        (catch Throwable _
                          nil)))
           eval-str (if operand-values
                      (str "\n\n\t("
                           (name (get-node-op node))
                           " "
                           (string/join " " operand-values)
                           ")")
                      "")
           grad-str (if grad
                      (str "\n\n\tGradient: " grad)
                      "")
           node-data-str (if-let [node-data (get-node-data node)]
                           (str "\n\n\tNode data:\n" (with-out-str (pprint node-data)))
                           "\n")
           context-str (if src-text
                         (str "\nIn \"" src-file-path "\":\n\n" src-text "\n")
                         "")]
       (throw
         (ex-info (str "Exception while " verb " " (get-node-name node) ":"
                       eval-str
                       grad-str
                       node-data-str
                       context-str)
                  (assoc creation-info
                    ::ranvier-runtime-exception true)
                  e))))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Reporters, datapoints, and loggers
;;
;; The frameowrk for logging information from a graph during optimization consists of four components:
;;   1. Datapoints - the data structure used to exchange reported data between different parts of the frame-
;;      work. These consist of values and additional metadata for that value (e.g. label, type, etc.)
;;   2. Reporter functions - functions that take a node value (or any value) and return a datapoint.
;;   3. Logging functions - functions that take a single datapoint or seq of datapoints and output them
;;      (e.g. printing them to an output stream or displaying a graph or pushing them to a web client).
;;   4. The `report` op - this takes an arbitrary number of node arguments, a reporter function, and a logger
;;      function. At graph runtime, it passes the node value to the reporter to construct a datapoint and
;;      then passes the resulting datapoint(s) to the logger.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; TODO: Consider this design more carefully. The ideal design would support:
;   - custom data point types
;   - custom formatters

(def- datapoint-types #{:nd :time :percent :integer :string})

(defn make-datapoint
  "Constructs a datapoint given:

    - `value` - The value of the datapoint (can be of arbitrary type or `nil`)
    - `type` - The type of the datapoint (must exist in the `datapoint-types` set immediately above
    - `label` - A string or keyword label for the datapoint
    - `info` - An optional map of arbitrary key:value pairs to associate with the datapoint. This can be used
      downstream by loggers to, e.g., determine the format of the datapoint."
  ([value label type]
   (make-datapoint value label type {}))
  ([value label type info]
   (when-not (datapoint-types type)
     (u/throw-str "Invalid datapoint type '" type "'. Valid types are: '" datapoint-types "'."))
   ^:datapoint {:value value
                :label label
                :type type
                :info info}))

(defn is-datapoint?
  "Returns `true` iff `o` is  ranvier datapoint object."
  [o]
  (boolean (:datapoint (meta o))))

(defn get-datapoint-value
  "Returns the value of a datapoint."
  [datapoint]
  (:value datapoint))

(defn get-datapoint-label
  "Returns the label for a datapoint."
  [datapoint]
  (:label datapoint))

(defn get-datapoint-type
  "Returns the type of a datapoint."
  [datapoint]
  (:type datapoint))

(defn get-datapoint-info
  "Returns the info map of a datapoint."
  [datapoint]
  (:info datapoint))

; TODO: Test
(defn- format-time
  "Given a timestamp (in nanoseconds), returns a string formatted nicely for human readability."
  [t-ns]
  (let [micros (float (/ t-ns 1000))
        ms (float (/ t-ns 1e6))
        s (float (/ t-ns 1e9))
        m (float (/ t-ns 6e10))
        h (float (/ t-ns 36e11))]
    (cond (>= h 1) (format "%dh %dm" (int h) (mod (int m) 60))
          (>= m 3) (format "%dm %ds" (int m) (mod (int s) 60))
          (>= s 1) (format (format "%%.%dfs" (- 2 (int (Math/log10 s)))) s)
          (>= ms 1) (format (format "%%.%dfms" (- 2 (int (Math/log10 ms)))) ms)
          (>= micros 1) (format (format "%%.%dfµs" (- 2 (int (Math/log10 micros)))) micros)
          (>= t-ns 1) (format (format "%%.%dfns" (- 2 (int (Math/log10 t-ns)))) t-ns)
          :else "0ns")))

(defn- format-float
  "Returns a string represetnation of `n` with the indicated number of `decimal-places` (default 2)."
  ([n]
   (format-float n 2))
  ([n decimal-places]
   (format (format "%%.%df" decimal-places) n)))

; String to print for `nil` datapoints.
(def- nil-placeholder "-")

(def- datapoint-type->formatter
  "Map of datapoint type key to a formatter function that must receive a datapoint and return a string.
   The formatters can assume the datapoint value is not `nil`. Used specifically by the print logger."
  {:nd #(-> % get-datapoint-value str)
   :time #(-> % get-datapoint-value format-time)
   :percent #(str (format-float (get-datapoint-value %)
                                (get (get-datapoint-info %) :decimal-places 1))
                  "%")
   :integer #(->> % get-datapoint-value (format "%d"))
   :string #(-> % get-datapoint-value str)})

; TODO: Add separator option to set how multiple datapoints are joined.
(def- print-logger-string (atom ""))
(defn make-print-logger
  "Returns a logger that prints reported datapoints to *out*. `opts` can include:

    - `:space` - include a space after the printed value
    - `:newline` - include a newline after the printed value

  The logger flushes on :newline."
  [& opts]
  (let [opts-set (into #{} opts)
        include-newline? (opts-set :newline)
        include-space? (opts-set :space)]
    (fn print-logger
      [datapoints]
      #_(with-open [pw (java.io.PrintWriter. System/out)])
      (if (is-datapoint? datapoints)
        (print-logger [datapoints])
        (let [s (->> (map (fn [datapoint]
                            (let [formatter (get datapoint-type->formatter (get-datapoint-type datapoint))
                                  formatted-value (if (nil? (get-datapoint-value datapoint))
                                                    nil-placeholder
                                                    (formatter datapoint))
                                  label (get-datapoint-label datapoint)
                                  label-str (if (keyword? label)
                                              (-> label
                                                  name
                                                  (string/replace "-" " ")
                                                  string/capitalize)
                                              (str label))]
                              (str label-str ": " formatted-value)))
                          datapoints)
                     (string/join "  ")
                     (#(if include-space? (str % "  ") %)))]
          (swap! print-logger-string str s)
          (when include-newline?
            (println @print-logger-string)
            (reset! print-logger-string "")))))))

(defn make-value-reporter
  "Returns a reporter function for tensor or numeric values. Optional `decimal-places` (default 4) can be
  provided and may be used by the logger for formatting."
  ([]
   (make-value-reporter ""))
  ([label]
   (make-value-reporter label 4))
  ([label decimal-places]
   (fn [val]
     (make-datapoint (if (and (m/array? val) (m/scalar? val))
                       (m/scalar->number val)
                       val)
                     label
                     :nd
                     {:decimal-places decimal-places}))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Optimizers
;;
;; An `optimizer` is any function that returns an `optimizer-result` (constructed using
;; `make-otpimizer-result`) and has the following arities:
;;
;;   [prev-optimizer-result]
;;   [prev-optimizer-result options]
;;   [graph input-map param-map state-map]
;;   [graph input-map param-map state-map options]
;;
;; where the arguments are as follows:
;;
;;   - `prev-optimizer-result` - an `optimizer-result` (typically that returned by the previous iteration)
;;   - `options`- a map of option names to values
;;   - `graph` - a computation graph
;;   - `input-map` - a map of input names to values for inputs the optimizer should *not* optimize; the
;;      values of these may change at most once during the optimization process: they may go from `nil` to
;;      having a value during the first iteration as a result of initialization
;;   - `param-map` - a map of input names to values for inputs the optimizer *should* optimize; these can
;;      change on every iteration as a result of the optimization algorithm
;;   - `state-map` - a map of input names to values for inputs that may change as a result of running the
;;      graph (e.g. a node receiving a recurrent connection may have a value that changes within an
;;      iteration, and the value at the end of one iteration may be its value at the start of the next
;;      iteration)
;;
;; The `optimizer` is expected to run one iteration per call. Multiple iteratiosn are achieved by passing
;; the `optimizer-result` from the previous call back to the `optimizer`. An `optimizer` can keep track of any
;; data it needs across iteratiosn by adding arbitrary data to the `optimization-result`s `iteration-data`
;; map. An `optimizer` should thus return in the `optimizer-result` all the information needed to run
;; additional iterations on a problem.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; TODO: Think about ways to simplify this.

(defn make-optimizer-result
  "Constructs an `optimizer-result` from:

    - `updated-graph` - a computation graph
    - `updated-input-map` - an updated map of input names to values
    - `updated-param-map` - an updated map of parameter names to values
    - `updated-state-map` - an updated map of state input names to values
    - `iteration-data` - a map of arbitrary key-value pairs that the optimizer can use to keep track of data
      from one iteration to the next.

  For typical optimization problems, the `param-map` will usually change from iteration to iteration,
  `state-map` may change from iteration to iteration, and the `input-map` may change only on the first
  iteration if some inputs are initialized at that time, and the `graph` itself will change only if it's
  dynamic in such a way that running it can change its structure."
  ([updated-graph updated-input-map updated-param-map updated-state-map]
   (make-optimizer-result updated-graph updated-input-map updated-state-map updated-param-map {}))
  ([updated-graph updated-input-map updated-param-map updated-state-map iteration-data]
   ^::optimizer-result {:params updated-param-map
                        :graph updated-graph
                        :inputs updated-input-map
                        :state updated-state-map
                        :iteration-data iteration-data}))

(defn validate-optimizer-result
  "Returns true iff `o` is a valid optimizer result. Otherwise, throws an Exception."
  [o]
  (when-not (-> o meta ::optimizer-result)
    (u/throw-str "Invalid optimizer result: '" o "'. Optimizers must return an object created by "
                 "`make-optimizer-result`.")))

(defn get-optimized-params
  "Given an `optimizer-result`, returns a map of parameter names to values."
  [optimizer-result]
  (validate-optimizer-result optimizer-result)
  (:params optimizer-result))

(defn get-optimized-inputs
  "Given an `optimizer-result`, returns a map of input names to values."
  [optimizer-result]
  (validate-optimizer-result optimizer-result)
  (:inputs optimizer-result))

(defn get-optimized-state
  "Given an `optimizer-result`, returns a map of state input names to values."
  [optimizer-result]
  (validate-optimizer-result optimizer-result)
  (:state optimizer-result))

(defn get-optimized-graph
  "Given an `optimizer-result`, returns the computation graph for the corresponding optimization problem."
  [optimizer-result]
  (validate-optimizer-result optimizer-result)
  (:graph optimizer-result))

(defn get-iteration-data
  "Given an `optimizer-result`, returns `iteration-data`, a map of arbitrary key/value pairs determined
  by the `optimizer`."
  [optimizer-result]
  (validate-optimizer-result optimizer-result)
  (:iteration-data optimizer-result))

(defn assoc-iteration-data
  "Given an `optimizer-result`, assoc's a new key-value (`k`-`v`) pair into the iteration-data map."
  [optimizer-result k v]
  (validate-optimizer-result optimizer-result)
  (assoc-in optimizer-result [:iteration-data k] v))

(defn dissoc-iteration-data
  "Given an `optimizer-result`, dissoc's `k` from its iteration-data map and returns the resulting new
  optimizer-result."
  [optimizer-result k]
  (validate-optimizer-result optimizer-result)
  (update optimizer-result :iteration-data dissoc k))

(defn update-iteration-data
  "Updates the value of key `k` in `optimizer-result`'s iteration-data to the value of
  `(apply f prev-value args)`."
  [optimizer-result k f & args]
  (validate-optimizer-result optimizer-result)
  (apply update optimizer-result :iteration-data update k f args))

(defn wrap-optimizer-for-reporting
  "Given an `optimizer`, returns a new optimizer that runs the original optimizer and optionally reports
  certain metrics after each iteration. In order for the returned optimizer to function correctly,
  the original `optimizer` must return in its `optimizer-result`'s `iteration-data` the following key/value
  pair:

    - `:report` - a seq of keywords indicating what metrics to report (see below)

  and optionally:

    - `:logger` - a logging function (defaults to a println logger)
    - `:epoch` - index of the current epoch for the just-completed iteration (should start at 1)
    - `:iteration` - index of the just-completed iteration (should start at 1)
    - `:batch-count` - number of batches per epoch
    - `:iterations` - total number of iterations that are expected to run

  The above data are required for reporting some metrics. Valid keys in `:report` are:

    - `:iteration` - requires `optimizer` to return `:iteration`
    - `:iteration-duration` - duration of the just-completed iteration
    - `:epoch` - requires `optimizer` to return `:epoch`
    - `:epoch-duration` - time since the start of the current epoch
    - `:batch` - index of the current batch in the epoch (requires `optimizer` to return `:iteration` and
      `:batch-count`)
    - `:elapsed-time` - time since the start of optimization
    - `:mean-iteration-duration` - mean time of all iterations since the start of optimization
    - `:epoch-time-remaining` - estimated time remaining in the current epoch (requires `optimizer` to return
      `:iteration` and `:batch-count`)
    - `:time-remaining` - estimated time remaining to complete all iterations; (requires `optimizer` to return
      `:iterations`)

  The `optimizer-result` returned by the wrapped optimizer will include a new `:ranvier.core/reporter-data`
  key with data needed for reporting."
  [optimizer]
  (fn [& optimizer-args]
    (let [prev-iter-reporter-data (when (<= (count optimizer-args) 2)
                                    (-> optimizer-args first get-iteration-data ::reporter-data))
          now (System/nanoTime)
          {:keys [epoch-start-ts start-ts prev-run-iterations]
           :or {epoch-start-ts now
                start-ts now
                prev-run-iterations 0}} prev-iter-reporter-data
          iteration-start-ts now
          optimizer-result (apply optimizer optimizer-args)
          this-iter-data (get-iteration-data optimizer-result)
          {:keys [iteration epoch batch-count logger report iterations]
           :or {logger (make-print-logger :newline)}} this-iter-data
          this-run-iteration-count (- iteration prev-run-iterations)
          batch (when (and iteration batch-count)
                  (int (inc (mod (dec iteration) batch-count))))
          epoch-start-ts (if (= batch 1)
                           iteration-start-ts
                           epoch-start-ts)
          now (System/nanoTime)
          total-elapsed-time (- now start-ts)
          mean-iteration-duration (when iteration
                                    (/ total-elapsed-time this-run-iteration-count))]
      (when (seq report)
        (->> report
             (mapv (fn [report]
                     (case report
                       :iteration (make-datapoint iteration :iteration :integer)
                       :iteration-duration (make-datapoint (- now iteration-start-ts)
                                                           :iteration-duration
                                                           :time)
                       :epoch (make-datapoint epoch :epoch :integer)
                       :batch (make-datapoint batch :batch :integer)
                       :epoch-duration (make-datapoint (- now epoch-start-ts)
                                                       :epoch-duration
                                                       :time)
                       :elapsed-time (make-datapoint total-elapsed-time :elapsed-time :time)
                       :mean-iteration-duration (make-datapoint mean-iteration-duration
                                                                :mean-iteration-duration
                                                                :time)
                       :epoch-time-remaining (make-datapoint (when (and batch-count batch)
                                                               (* (- batch-count batch)
                                                                  mean-iteration-duration))
                                                             :epoch-time-remaining
                                                             :time)
                       :time-remaining (make-datapoint (when (and iteration iterations)
                                                         (* (- iterations this-run-iteration-count)
                                                            mean-iteration-duration))
                                                       :time-remaining
                                                       :time)
                       (u/throw-str "Invalid optimizer reporting metric: '" report "'."))))
             logger))
      (assoc-iteration-data optimizer-result ::reporter-data
                            {:epoch-start-ts epoch-start-ts
                             :start-ts start-ts
                             :prev-run-iterations prev-run-iterations
                             :iterations-completed-this-run this-run-iteration-count}))))

(defn reset-optimizer-reporting
  "Given an `optimizer-result` produced by an optimizer wrapped by `wrap-optimizer-for-reporting`, returns
  a new optimizer-result in which reporting data has been reset. Useful for 'resetting' reporting when
  optimization is restarted after being previously stopped."
  [optimizer-result]
  (update-iteration-data optimizer-result ::reporter-data
                         (fn [prev-reporter-data]
                           (let [prev-run-iterations (get prev-reporter-data :prev-run-iterations 0)
                                 iterations-completed-this-run (get prev-reporter-data
                                                                    :iterations-completed-this-run 0)]
                             {:prev-run-iterations (+ prev-run-iterations iterations-completed-this-run)}))))

(defn- get-current-batch-input-map
  "Given:
    - `input-map` - a map of input node names to values for graph inputs
    - `batch-stats` - map returned by `get-batch-stats`
    - `iteration` - iteration index
  returns an updated-input map where batched inputs have been updated with views over the current batch."
  [input-map batch-stats iteration]
  (let [{:keys [batch-count batch-axis-size batch-size batch-inputs]} batch-stats]
    (if (= batch-count 1)
      input-map
      (let [batch-index (mod (dec iteration) batch-count)
            this-batch-start (* batch-index batch-size)
            this-batch-end (clojure.core/min (+ this-batch-start batch-size)
                                             batch-axis-size)]
        (reduce (fn [batch-input-map [input-name batch-axis]]
                  (update batch-input-map input-name
                    #(m/select-axis-range % batch-axis [this-batch-start this-batch-end])))
                input-map
                batch-inputs)))))

(defn- get-batch-stats
  "Given an `input-map` and `options` from `gradient-descent-optimizer`, returns a map with the follwing keys:
    - `:batch-count` - number of batches per epoch (i.e. number of batches in the entire data set)
    - `:batch-axis-size` - total size of the axis that's split into batches
       [batch-size * (batch-count - 1) < batch-axis-size <= batch-size * batch-count], since
       0 < last batch size <= batch-size.
    - `:batch-size` - size of a batch
    - `:batch-inputs` - map of input name to the axis to split into batches
  will return {:batch-count 1} if not batching."
  [input-map options]
  (if-not (or (:batch-inputs options) (:batch-size options))
    {:batch-count 1}
    (let [batch-inputs (or (:batch-inputs options)
                           (->> (keys input-map)
                                (map #(vector % 0))
                                (into {})))
          batch-size (or (:batch-size options) 192)
          input->batch-axis-size (->> (map (fn [[input-name batch-axis]]
                                             [input-name
                                              (m/dimension-count (get input-map input-name) batch-axis)])
                                           batch-inputs)
                                      (into {}))
          batch-axis-sizes (distinct (vals input->batch-axis-size))
          _ (when (> (count batch-axis-sizes) 1)
              (u/throw-str "Cannot batch inputs of different sizes. Batch axes: '" batch-inputs "'. "
                           "Batch axis sizes: '" input->batch-axis-size "'."))
          batch-axis-size (first batch-axis-sizes)]
      {:batch-count (Math/ceil (/ batch-axis-size batch-size))
       :batch-axis-size batch-axis-size
       :batch-size batch-size
       :batch-inputs batch-inputs})))

(defn- get-starting-gradient-descent-iteration-data
  "Given a `param-map` from `gradient-descent-optimizer`, returns a newly initialized map of iteration-data."
  [param-map]
  (let [param-name->zero (->> (keys param-map)
                              (map #(vector % (m/array 0)))
                              (into {}))]
    {:smoothed-grads param-name->zero
     :smoothed-squared-grads param-name->zero
     :iteration 0
     :epoch 1}))

(defn- base-gradient-descent-optimizer
  "Contains the meat of the gradient-descent implementation. See `gradient-descent-optimizer` below, which
  is merely this function wrapped for reporting. The returned `optimizer-result` includes the following data:

    - `report`- passed through from `options`
    - `logger` - passed through from `options`
    - `iterations` - passed through from `options`
    - `batch-count` - number of batches per epoch
    - `batch-axis-size` - size of the axis being batched
    - `batch-inputs` - passed through from `options` (or else the default if `batch-size` is included in `options`.
    - `smoothed-grads` - map of param name to the exponentionally weighted moving average (EWMA) of its gradient
    - `grads` - map of param name to gradient
    - `smoothed-squared-grads` - map of param name to the EWMA of the squared gradient
    - `iteration` - iteration index (starting at 1)
    - `epoch` - epoch index (starting at 1)"
  ([prev-optimizer-result]
   (base-gradient-descent-optimizer prev-optimizer-result {}))
  ([prev-optimizer-result options]
   (base-gradient-descent-optimizer (get-optimized-graph prev-optimizer-result)
                                    (get-optimized-inputs prev-optimizer-result)
                                    (get-optimized-params prev-optimizer-result)
                                    (get-optimized-state prev-optimizer-result)
                                    (get-iteration-data prev-optimizer-result)
                                    options))
  ([graph input-map param-map state-map]
   (base-gradient-descent-optimizer graph input-map param-map state-map {}))
  ([graph input-map param-map state-map options]
   (base-gradient-descent-optimizer
     graph
     input-map
     param-map
     state-map
     (get-starting-gradient-descent-iteration-data param-map)
     options))
  ([graph input-map param-map state-map prev-iteration-data options]
   (u/check-key-validity options [] [:learning-rate :friction :rms-friction :eps :batch-inputs
                                     :batch-size :iterations :report :logger])
   (when-let [common-inputs (seq (set/intersection (into #{} (keys input-map))
                                                   (into #{} (keys param-map))))]
     (u/throw-str "`param-map` and `input-map` contain common inputs: '" (vec common-inputs) "'."))
   (let [{:keys [learning-rate friction]} options
         eps (m/array (get options :eps 1e-8))
         learning-rate (m/array (get options :learning-rate 0.001))
         {prev-smoothed-grads :smoothed-grads
          prev-smoothed-squared-grads :smoothed-squared-grads
          last-iteration :iteration} prev-iteration-data
         this-iteration (inc last-iteration)
         {:keys [batch-count batch-axis-size batch-size batch-inputs]
          :as batch-stats} (get-batch-stats input-map options)
         this-epoch (inc (int (Math/floor (/ (dec this-iteration) batch-count))))
         initialize? (= this-iteration 1)
         batch-input-map (get-current-batch-input-map input-map batch-stats this-iteration)
         values (forward graph (merge batch-input-map param-map state-map))
         param-grads (backward graph values (keys param-map))
         rms-friction-num (:rms-friction options)
         smoothed-squared-grads (if-not rms-friction-num
                                  prev-smoothed-squared-grads
                                  (let [friction (m/array rms-friction-num)
                                        accel (m/array (- 1 rms-friction-num))]
                                    (->> (map (fn [[param-name this-step-grad]]
                                                [param-name
                                                 (m/add (m/mul (get prev-smoothed-squared-grads param-name)
                                                               friction)
                                                        (m/mul (m/pow this-step-grad (m/array 2))
                                                               accel))])
                                              param-grads)
                                         (into {}))))
         friction-num (:friction options)
         smoothed-grads (if-not friction-num
                          param-grads
                          (let [friction (m/array friction-num)
                                accel (m/array (- 1 friction-num))]
                            (->> (map (fn [[param-name this-step-grad]]
                                        [param-name
                                         (m/add (m/mul (get prev-smoothed-grads param-name)
                                                       friction)
                                                (m/mul this-step-grad accel))])
                                      param-grads)
                                 (into {}))))
         grad-bias-correction (when friction-num
                                (m/array (- 1 (Math/pow friction-num this-iteration))))
         rms-bias-correction (when rms-friction-num
                               (m/array (- 1 (Math/pow rms-friction-num this-iteration))))
         updated-param-map (->> (map (fn [[param-name old-value]]
                                       (when-not old-value
                                         (u/throw-str "Missing parameter value for '" param-name "'. Does
                                                      it need to be initialized?"))
                                       (let [grad (get smoothed-grads param-name)
                                             corrected-grad (if grad-bias-correction
                                                              (m/div grad grad-bias-correction)
                                                              grad)
                                             step (if-not rms-friction-num
                                                    corrected-grad
                                                    (->> (get smoothed-squared-grads param-name)
                                                         (#(m/div % rms-bias-correction))
                                                         m/sqrt
                                                         (m/add eps)
                                                         ; TODO: Make sure this should be `corrected-grad` and not `grad`.
                                                         (m/div corrected-grad)))]
                                         [param-name
                                          (m/sub old-value
                                                 (m/mul learning-rate step))]))
                                     (if initialize?
                                       (select-keys values (keys param-map))
                                       param-map))
                                (into {}))
         updated-input-map (if-not initialize?
                             input-map
                             (let [initialized-input-names (keep (fn [[input-name input-value]]
                                                                   (when (nil? input-value)
                                                                     input-name))
                                                                 input-map)]
                               (merge input-map
                                      (select-keys values initialized-input-names))))
         updated-state-map (select-keys values (keys state-map))]
     (make-optimizer-result
       graph
       updated-input-map
       updated-param-map
       updated-state-map
       (assoc (select-keys options [:report :iterations :logger])
         :batch-count batch-count
         :batch-axis-size batch-axis-size
         :batch-inputs batch-inputs
         :batch-size batch-size
         :smoothed-grads smoothed-grads
         :grads param-grads
         :smoothed-squared-grads smoothed-squared-grads
         :iteration this-iteration
         :epoch this-epoch)))))

(def gradient-descent-optimizer
  "An optimizer that minimizes the value produced by running the `graph` using stadnard gradient descent
  methods. `options` can include:

    - `learning-rate` - gradient multipler to use for calculating updated parameter values (defaults to
      0.001). Each iteration updates a given parameter X by step-size, where step-size[X] = dX * learning-rate
      and dX is the gradient of X. Thus, given that X[t] is the value of parameter X on iteration t,
      X[t+1] = X[t] - step-size * dX.
    - `report` - a seq of keys for reporting (see `wrap-optimizer-for-reqporting`).
    - `logger` - a logger for reporting
    - `iterations` - the target number of iterations (used only for reporitng purposes)
    - `batch-size` - positive integer indicating the size of a batch (defaults to 192 if `batch-inputs` is
      provided
    - `batch-inputs` - map of input name to axes to batch along. For instance, if input :x is a matrix with
      examples in columns and input :y is a vector of values for each example and you
      want to run a batch of examples on each iteration, `batch-inputs` should be {:x 1 :y 0}. (Defaults to
      batching along the 0th dimension if `batch-size` is provided.)
    - `friction` - 'friction' value used for smoothing gradients. If omitted, no smoothing is done, and
      step-size[X,t] = dX[t] * learning-rate for iteration t as described above. If specified, then
      step-size[X,t] = dXsmoothed[t] * learning-rate / (1 - friction ^ t), where dXsmoothed[t] is the
      expoentionally weighted moving average of dX over iterations 1...t.:

        dXsmoothed[t] = dXsmoothed[t - 1] * friction + (1 - friction) * dX[t]

      (Recommended value: 0.9).
    - `eps` - a small value used for numerical stability when using RMSProp. See below. (Defaults to 1e-8.)
    - `rms-friction` - 'friction' to use for RMSProp; if ommitted, RMSProp is not used. If included:

         step-size[X,t] = learning-rate * dXsmoothed[t] / (sqrt(bias-corrected-ewma-squared-grads[X,t]) + eps)

      where:

         bias-corrected-ewma-squared-grads[X,t] = ewma-squared-grads[X,t] / (1 - rms-friction ^ t)

      and ewma-squared-grad[X,t] is the exponentially weighted moving average of the squared gradients over
      1...t:

         ewma-squared-grads[X,t] = ewma-squared-grads[X, t - 1] * friction + dXsmoothed[t] ^ 2 * (1 - friction)

      (Recommended value: 0.999)."
  (wrap-optimizer-for-reporting base-gradient-descent-optimizer))

(defn- optimize-recursive
  [optimizer prev-optimizer-result options iterations]
  (if (= iterations 0)
    prev-optimizer-result
    (recur optimizer
           (optimizer prev-optimizer-result options)
           options
           (dec iterations))))

(defn optimize
  "Applies `optimizer` to the remaining arguments iteratively for a number of iterations determined by
  the value of `(:iterations options)` (defaults to 1 iteraton). Blocks the calling thread until all
  iterations are complete. Returns the optimizer-result of the last iteration."
  ([optimizer prev-optimizer-result]
   (optimize optimizer prev-optimizer-result {}))
  ([optimizer prev-optimizer-result options]
   (optimize-recursive optimizer
                       (optimizer prev-optimizer-result options)
                       options
                       (dec (get options :iterations 1))))
  ([optimizer graph input-map param-map state-map]
   (optimize optimizer graph input-map param-map state-map {}))
  ([optimizer graph input-map param-map state-map options]
   (let [iterations (get options :iterations 1)]
     (if (= iterations 0)
       (make-optimizer-result graph input-map param-map state-map)
       (optimize-recursive optimizer
                           (optimizer graph input-map param-map state-map options)
                           options
                           (dec iterations))))))

(defn- optimize-async-helper
  [optimizer & args]
  (let [options (last args)
        last-result (atom (if (= (count args) 2)
                            (first args)
                            (apply make-optimizer-result (take 4 args))))
        stop? (atom false)
        iterations (:iterations options)]
    (async/thread
      (when (or (not iterations) (> iterations 0))
        (reset! last-result (apply optimizer args)))
      (loop [iterations-left (when iterations
                               (dec iterations))]
        (when (and (or (not iterations) (> iterations-left 0))
                   (not @stop?))
          (reset! last-result (optimizer @last-result options))
          (recur (when iterations-left
                   (dec iterations-left))))))
    (fn stop []
      (reset! stop? true)
      @last-result)))

(defn optimize-async
  "Applies `optimizer` to the remaining arguments iteratively for a number of iterations determined by
  the value of `(:iterations options)` (defaults to infinite iteratons). Launches a new thread in which
  optimization is performed and immediately returns a function, `stop`. Calling `(stop)` will immediately
  return the `optimizer-result` of the last iteration and terminate the optimization process after the next
  iteration is complete."
  ([optimizer prev-optimizer-result]
   (optimize-async optimizer prev-optimizer-result {}))
  ([optimizer prev-optimizer-result options]
   (optimize-async-helper optimizer prev-optimizer-result options))
  ([optimizer graph input-map param-map state-map]
   (optimize-async optimizer graph input-map param-map state-map {}))
  ([optimizer graph input-map param-map state-map options]
   (optimize-async-helper optimizer graph input-map param-map state-map options)))

(defn simple-optimize
  "Performs optimization of computation `graph`. `options` may include:

    - `:optimizer` (default `gradient-descent-optimizer`) - the optimizer function to use or else a keyword
      for commonly used optimization strategies
    - `:inputs` (default `nil`) - map of input name to a fixed value for inputs not being optimized
    - `:params` (default all graph inputs) - map of input name to starting value for parameters to be optimized
    - `:state` (default `nil`) - map of inputs name to starting value for inputs whose values will change as
       a result of running the graph but which should not be optimized
    - `:async` (default `false`) - boolean indicating if optimization should be run in a new thread
    - `:iterations` (default 1 when `async` is `false` or infinite when `async` is true) - number of
       iterations to run
    - `:hyperparams` (default `nil`) - map of options that get passed onto the optimizer
    - `:full-result` (default `false`) - boolean indicating whether to return the full `optimizer-result`
      from the last iteration (when `true`) or just the optimized parameter map; if `:async` is `true` this
      applies to the return value of the returned function, rather than to the immediate return value

  `:optimizer` can be an optimizer function or else one of the following keywords:

    - `sgd` - stochastic gradient descent; simple gradient descent with a batch-size of 1 over all inputs
      (if you want to run all inputs per iteration, then leave `:optimizer` `nil`)
    - `:batch-gd` - batch gradient descent with a `:batch-size` of 192 over all inputs
    - `:momentum` - gradient descent with momentum
    - `:rms-prop` - RMSProp
    - `:adam` - the Adam algorithm (RMSProp + momentum in this implementation)"
  ([graph]
   (simple-optimize graph {}))
  ([graph options]
   (u/check-key-validity options [] [:inputs :params :state :optimizer :async :iterations :hyperparams
                                     :full-result :prev-result])
   (let [input-map (or (:inputs options)
                       {})
         provided-inputs-set (set (keys input-map))
         all-inputs-set (set (get-all-graph-input-names graph))
         param-map (or (:params options)
                       (->> (set/difference all-inputs-set
                                            provided-inputs-set)
                            (map #(vector % (m/array 0)))
                            (into {})))
         state-map (or (:state options)
                       (->> (set/difference all-inputs-set
                                            provided-inputs-set
                                            (set (keys param-map)))
                            (map #(vector % (m/array 0)))
                            (into {})))
         optimizer-algorithm (:optimizer options)
         optimizer (if (fn? optimizer-algorithm)
                     optimizer-algorithm
                     gradient-descent-optimizer)
         async? (:async options)
         run-optimization (if async?
                            optimize-async
                            optimize)
         base-optimizer-options (case optimizer-algorithm
                                  :sgd {:batch-size 1}
                                  :batch-gd {:batch-size 192}
                                  :rms-prop {:rms-friction 0.999}
                                  :adam {:rms-friction 0.999
                                         :friction 0.9}
                                  :momentum {:friction 0.9}
                                  {})
         optimizer-options (merge base-optimizer-options
                                  (:hyperparams options)
                                  (select-keys options [:iterations]))
         optimizer-result (if-let [prev-result (:prev-result options)]
                            (run-optimization optimizer prev-result optimizer-options)
                            (run-optimization optimizer graph input-map param-map state-map optimizer-options))
         full-result? (:full-result options)]
     (cond full-result? optimizer-result
           async? (fn []
                    (get-optimized-params (optimizer-result)))
           :else (get-optimized-params optimizer-result)))))

;; TODO:
;; Improvements
;; - Add better error message when axis is out of bounds or invalid for all ops that take axes
;; - Consider allowing initialization function to be passed in via the input map rather than being stored on
;;   the node
;; - Figure out an optimization for ops where backward and forward functions share calculations (currently
;;   these shared calculations get repeated)
;; - Remove `forward-fn` and `backward-fn` from node structure and instead look these up in table (this would
;;   require adding a method to add a new function to the table)
;; - Simplify optimization framework. Right now the interface is too complicated for anyone to correctly
;;   write a pluggable optimization function. One option would be to allow a pluggable function that just
;;   takes the forward node values and returns the updated values for the next forward iteration.
;; - Figure out what `optimize` should do when iterations = 0? Just return `nil` instead of constructing
;;   the optimizer result?

;; Additional features:
;; - Add ops:
;;     permute
;;     argmin-along
;;     argmax-along
;;     (sqrt)
;;     (cbrt)
;;     (log10)
;;     (fill)
;;     (clamp)
;;     (distance)
;;     (floor)
;;     (ceil)
;;     (rotate)
;;     (round)
;;     (signum)
;;     (swap-rows)
;; - Computation graph optimization - e.g. any nodes downstream of only constants can be precomputed
;;   (generating a new constant node)
;; - Think of better way to handle purely non-backpropagating ops - these can just be functions that get
;;   run on every run of the graph (like support anonymous ops)
;; - Serialization (of graphs? optimization-results? parameters? all of the above?)
;; - Learning rate decay
;; - :epoch-count option for optimizer reporter

;; Quality:
;; - Fix fork and optimize it (right now it is doing a lot of unnecessary calculation)
;; - Add propagation tests (i.e. on complex graphs) for remaining ops (forward/backward tests) - fill,
;;   logical, comparison, trig, hyperbolic, rehaping ops
;; - Tests of:
;;     - runtime initialization
;;     - all functions that aren't yet unit tested
;;     - input
;;     - all optimizer functionality
;; - Improve testing of exceptions (i.e. actually validate that the thrown Exception is the correct one)
;; - Tests for:
;;     - partition-along op
;; - Move some of the validity checks to :pre or :post conditions?
;; - Move max-mask and min-mask to tensure? Move forward tensor function to tensure?
;; - Make sure can't provide an input value for a const
